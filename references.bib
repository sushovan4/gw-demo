@article{rosenblatt1958perceptron,
  added-at = {2017-07-19T15:29:59.000+0200},
  author = {Rosenblatt, F.},
  biburl = {https://www.bibsonomy.org/bibtex/214ee8da21c66cd4d00d7ab6eca2d96a9/andreashdez},
  citeulike-article-id = {13697582},
  citeulike-linkout-0 = {http://dx.doi.org/10.1037/h0042519},
  doi = {10.1037/h0042519},
  interhash = {dc0cef9dc06033a04f525efdcde7a660},
  intrahash = {14ee8da21c66cd4d00d7ab6eca2d96a9},
  issn = {0033-295X},
  journal = {Psychological Review},
  keywords = {imported},
  number = 6,
  pages = {386--408},
  posted-at = {2016-05-02 20:23:36},
  priority = {2},
  timestamp = {2017-07-19T15:31:02.000+0200},
  title = {{The perceptron: A probabilistic model for information storage and organization in the brain.}},
  url = {http://dx.doi.org/10.1037/h0042519},
  volume = 65,
  year = 1958
}
@article{knuth84,
  author = {Knuth, Donald E.},
  title = {Literate Programming},
  year = {1984},
  issue_date = {May 1984},
  publisher = {Oxford University Press, Inc.},
  address = {USA},
  volume = {27},
  number = {2},
  issn = {0010-4620},
  url = {https://doi.org/10.1093/comjnl/27.2.97},
  doi = {10.1093/comjnl/27.2.97},
  journal = {Comput. J.},
  month = may,
  pages = {97â€“111},
  numpages = {15}
}
@book{ita,
author = {Cormen, Thomas H. and Leiserson, Charles E. and Rivest, Ronald L. and Stein, Clifford},
title = {Introduction to Algorithms, Third Edition},
year = {2009},
isbn = {0262033844},
publisher = {The MIT Press},
edition = {3rd},
abstract = {If you had to buy just one text on algorithms, Introduction to Algorithms is a magnificent choice. The book begins by considering the mathematical foundations of the analysis of algorithms and maintains this mathematical rigor throughout the work. The tools developed in these opening sections are then applied to sorting, data structures, graphs, and a variety of selected algorithms including computational geometry, string algorithms, parallel models of computation, fast Fourier transforms (FFTs), and more. This book's strength lies in its encyclopedic range, clear exposition, and powerful analysis. Pseudo-code explanation of the algorithms coupled with proof of their accuracy makes this book is a great resource on the basic tools used to analyze the performance of algorithms.}
}
@book{Kleinberg+Tardos,
  added-at = {2009-03-18T13:40:43.000+0100},
  author = {Kleinberg, Jon and Tardos, \'Eva},
  biburl = {https://www.bibsonomy.org/bibtex/21f85484640a0cdcea4a1fbdd6bbc281d/om},
  interhash = {f4e0ffcd4e9b25e3ec4b305e12785cae},
  intrahash = {1f85484640a0cdcea4a1fbdd6bbc281d},
  keywords = {analysis mining opinion sentiment},
  publisher = {Addison Wesley},
  timestamp = {2009-03-18T13:41:48.000+0100},
  title = {Algorithm Design},
  year = 2006
}
@book{foma,
author = {Mohri, Mehryar and Rostamizadeh, Afshin and Talwalkar, Ameet},
title = {Foundations of Machine Learning},
year = {2018},
isbn = {0262039400},
publisher = {The MIT Press},
edition = {2nd},
abstract = {A new edition of a graduate-level machine learning textbook that focuses on the analysis and theory of algorithms. This book is a general introduction to machine learning that can serve as a textbook for graduate students and a reference for researchers. It covers fundamental modern topics in machine learning while providing the theoretical basis and conceptual tools needed for the discussion and justification of algorithms. It also describes several key aspects of the application of these algorithms. The authors aim to present novel theoretical tools and concepts while giving concise proofs even for relatively advanced topics. Foundations of Machine Learning is unique in its focus on the analysis and theory of algorithms. The first four chapters lay the theoretical foundation for what follows; subsequent chapters are mostly self-contained. Topics covered include the Probably Approximately Correct (PAC) learning framework; generalization bounds based on Rademacher complexity and VC-dimension; Support Vector Machines (SVMs); kernel methods; boosting; on-line learning; multi-class classification; ranking; regression; algorithmic stability; dimensionality reduction; learning automata and languages; and reinforcement learning. Each chapter ends with a set of exercises. Appendixes provide additional material including concise probability review. This second edition offers three new chapters, on model selection, maximum entropy models, and conditional entropy models. New material in the appendixes includes a major section on Fenchel duality, expanded coverage of concentration inequalities, and an entirely new entry on information theory. More than half of the exercises are new to this edition.}
}