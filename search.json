[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning",
    "section": "",
    "text": "Welcome",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#who-is-the-instructor",
    "href": "index.html#who-is-the-instructor",
    "title": "Machine Learning",
    "section": "Who is the instructor?",
    "text": "Who is the instructor?\nName: Dr. Sushovan Majhi\nEmail: s.majhi@gwu.edu\nHomepage: smajhi.com",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "notes/regression.html",
    "href": "notes/regression.html",
    "title": "1  Linear Regression",
    "section": "",
    "text": "1.1 Computational Problems\nLet us first define a computational problem. A (computational) problem is a well-specified task stated in general terms, specifying the desired output for any acceptable input to the problem.\nFor example, sorting a list of numbers is a computational problem. It stipulates that an acceptable input must be a list or collection of real numbers, and the desired output must be a list with the same numbers but arranged in increasing (equivalently decreasing) order.\nKeep in mind that the task of sorting a specific list (e.g. [0, -1, 2, 0]) should not be called a computational problem, since it is not given in general terms. When considering such a specific input to the problem, we may rather call it an instance of the sorting problem as discussed in the previous paragraph.\nFormally, we define the sorting problem as follows:\nWe use algorithms to solve computational problems.",
    "crumbs": [
      "Supervised",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "notes/regression.html#computational-problems",
    "href": "notes/regression.html#computational-problems",
    "title": "1  Linear Regression",
    "section": "",
    "text": "Input: an array of numbers A=\\langle a_1, a_2, \\ldots, a_n\\rangle.\nOutput: A reordering or permutation \\langle a_1', a_2', \\ldots, a_n'\\rangle of the array A such that a_1'\\leq a_2'\\leq\\ldots\\leq a_n.\n\n\nExercise 1.1 Name five computational problems that we come across in our daily lives.\n\n\n\n\n\n\n\nSolution:\n\n\n\n\n\n\nadding two numbers\nsorting an array\nfinding the minimum value is an array\nsearching a query value in an array\nfinding the shortest path between two vertices in a graph",
    "crumbs": [
      "Supervised",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "notes/regression.html#what-is-an-algorithm",
    "href": "notes/regression.html#what-is-an-algorithm",
    "title": "1  Linear Regression",
    "section": "1.2 What is an Algorithm?",
    "text": "1.2 What is an Algorithm?\nAn algorithm is any well-defined computational procedure that takes some value (or set of values) as input and produces some value as output. Thus, an algorithm is a sequence of (finite) computational steps that transform its input to an output.\nWe may have different algorithms to solve the same computational problem. However, they often differ dramatically in their efficiency.\n\nWriting Pseudocode\nIn this course, we mostly rely on pseudocodes for describing algorithms. Their implementation in Python will be an less significant step. Pseudocode delineates the logic of an algorithm without explicitly using the syntax of a programming language.\n\n\n\n\n\n\nConvention\n\n\n\n\n\nWe use the following conventions in our pseudocode:\n\nIndentation indicates block structure.\nThe looping constructs while, for, and repeat-until and the if-else conditional construct have interpretations similar to Python.\nThe symbol // indicates that the remainder of the line is a comment.\nVariables (such as i, j , and key) are local to the given procedure. We won’t use global variables without explicit indication.\nWe access array elements by specifying the array name followed by the index in square brackets. For example, A[i] indicates the ith element of the array A.\nWe typically organize compound data into objects, which are composed of attributes.\nWe pass parameters to a procedure by value: the called procedure receives its own copy of the parameters, and if it assigns a value to a parameter, the change is not seen by the calling procedure\nA return statement immediately transfers control back to the point of call in the calling procedure.\nThe boolean operators and and or are short circuiting.\n\n\n\n\nWe give a few examples first.",
    "crumbs": [
      "Supervised",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "notes/regression.html#example-algorithms",
    "href": "notes/regression.html#example-algorithms",
    "title": "1  Linear Regression",
    "section": "1.3 Example Algorithms",
    "text": "1.3 Example Algorithms\nLet us develop and analyze a few example algorithms.\n\nSUM-ARRAY\nDescription: SUM-ARRAY is a computational problem that computes the sum of the n numbers in array A[0:n-1].\n\nInput: An array or list A[0:n-1] containing n numbers.\nOutput: The sum of the numbers in A[0:n-1].\n\n\nExample 1.1 If the input array is A = &lt;1, 0, -1, 2&gt;, then the return must be 1 + 0 + (-1) + 2 = 2.\n\n\n\n\n\n\n\nPseudocode\n\n\n\n\n\n\n\n\\begin{algorithm} \\caption{SUM-ARRAY} \\begin{algorithmic} \\Procedure{SUM-ARRAY}{$A[0:n-1]$} \\State $sum = 0$ \\For{$i = 0$ \\To $n-1$} \\State $sum = sum + A[i]$ \\State $i = i + 1$ \\EndFor \\Return $sum$ \\EndProcedure \\end{algorithmic} \\end{algorithm}\n\n\n\n\n\n\nExercise 1.2 (Dry)-run SUM-ARRAY on the input A=&lt;-2, 3, 5, 0, -1&gt;.\n\n\nExercise 1.3 Implement SUM-ARRAY in Python.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ndef SUM_ARRAY(arr):\n    sum = 0\n    n = len(arr)\n    for i in range(n):\n        sum = sum + arr[i]\n    return sum\n\n\n\n\n\n\nSUM-ARRAY-POS\nDescription: SUM-ARRAY-POS is a computational problem that computes the sum of the positive numbers in an array A[0:n-1] of size n.\n\nInput: An array or list A[0:n-1] containing n numbers.\nOutput: The sum of the positive numbers in A.\n\n\nExample 1.2 If the input is the array A = &lt;1, 0, -1, 2&gt;, then the return will be 1 + 2 = 3.\n\n\n\n\n\n\n\nPseudocode\n\n\n\n\n\n\n\n\\begin{algorithm} \\caption{SUM-ARRAY-POS} \\begin{algorithmic} \\Procedure{SUM-ARRAY-POS}{$A[0:n-1]$} \\State $sum = 0$ \\For{$i = 0$ \\To $n-1$} \\If{$A[i] &gt; 0$} \\State $sum = sum + A[i]$ \\EndIf \\State $i = i + 1$ \\EndFor \\Return $sum$ \\EndProcedure \\end{algorithmic} \\end{algorithm}\n\n\n\n\n\n\nExercise 1.4 (Dry)-run SUM-ARRAY-POS on the input A=&lt;-2, 3, 5, 0, -1&gt;.\n\n\nExercise 1.5 Implement SUM-ARRAY-POS in Python.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ndef SUM_ARRAY_POS(arr):\n    sum = 0\n    n = len(arr)\n    for i in range(n):\n      if arr[i] &gt; 0:\n        sum = sum + arr[i]\n    return sum\n\n\n\n\n\n\nSEARCH-ARRAY\nDescription: SEARCH-ARRAY is a computational problem that finds an element in an array.\n\nInput: An array or list A[0:n-1] containing n numbers and a query value x.\nOutput: An first index i such that A[i]=x, otherwise -1.\n\n\nExample 1.3 If the inputs are the array A = &lt;1, 0, -1, 2&gt; and the value x = 0, then the return will be 1.\n\n\n\n\n\n\n\nPseudocode\n\n\n\n\n\n\n\n\\begin{algorithm} \\caption{SEARCH-ARRAY} \\begin{algorithmic} \\Procedure{SEARCH-ARRAY}{$A[0:n-1]$, $x$} \\For{$i = 0$ \\To $n-1$} \\If{$A[i] = x$} \\Return $i$ \\EndIf \\State $i = i + 1$ \\EndFor \\Return $-1$ \\EndProcedure \\end{algorithmic} \\end{algorithm}\n\n\n\n\n\n\nExercise 1.6 (Dry)-run SEARCH-ARRAY on the input A=&lt;-2, 3, 5, 0, -1&gt; and x=4.\n\n\nExercise 1.7 Implement SEARCH-ARRAY in Python.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ndef SEARCH_ARRAY(arr, x):\n    n = len(arr)\n    for i in range(n):\n        if arr[i] == x:\n          return i\n    return -1\n\n\n\n\n\nExercise 1.8 (MIN-ARRAY) Develop an algorithm to find the minimum value in an array A[0:n-1].\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\\begin{algorithm} \\caption{MIN-ARRAY} \\begin{algorithmic} \\Procedure{MIN-ARRAY}{$A[0:n-1]$} \\State $min=A[0]$ \\For{$i = 1$ \\To $n-1$} \\If{$A[i] &lt; min$} \\State $min=A[i]$ \\EndIf \\State $i = i + 1$ \\EndFor \\Return $min$ \\EndProcedure \\end{algorithmic} \\end{algorithm}\n\n\n\n\n\n\nExercise 1.9 (SUM-MATRIX) Develop an algorithm for summing all the elements in a square matrix A=[a_{i,j}] of size n\\times n.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\\begin{algorithm} \\caption{SUM-MATRIX} \\begin{algorithmic} \\Procedure{SUM-MATRIX}{$A[0:n-1][0:n-1]$} \\State $min=0$ \\For{$i = 0$ \\To $n-1$} \\For{$j = 0$ \\To $n-1$} \\State $sum=sum + A[i][j]$ \\State $j = j + 1$ \\EndFor \\State $i = i + 1$ \\EndFor \\Return $sum$ \\EndProcedure \\end{algorithmic} \\end{algorithm}",
    "crumbs": [
      "Supervised",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "notes/regression.html#analyzing-algorithms",
    "href": "notes/regression.html#analyzing-algorithms",
    "title": "1  Linear Regression",
    "section": "1.4 Analyzing Algorithms",
    "text": "1.4 Analyzing Algorithms\nAnalyzing an algorithm means to predict the computational resources (e.g. computational time, memory) needed to run the algorithm on different instances or inputs. But how do we compute the running time of an algorithm?\n\nExercise 1.10 Do you think it is easy to compute the time taken by an algorithm?\n\nAlthough such a task is almost impossible, we can still pretty easily analyze the computational time of an algorithm—at least conceptually!\nLet’s use first review our basic intuition behind running-time of an algorithm:\n\nit should depend on the input size\nthe larger the input size, the more time needed\nrunning-time should be machine and compiler dependent\n\nIn order to make the task easier, we sum the running times for each statement executed. We usually denote the running time of an algorithm on an input of size n by T(n).\n\nSUM-ARRAY\nFor each line in the algorithm, we assign a cost symbolically, then count how many times the line is visited. This is summarized in the following table.\n\n\n\nLine\nCost\nTimes\n\n\n\n\n2\nc_1\n1\n\n\n3\nc_2\nn\n\n\n4\nc_3\nn\n\n\n5\nc_4\nn\n\n\n6\nc_5\nn\n\n\n7\nc_5\n1\n\n\n\nSo, T(n)=(c_1 + c_5) + (c_2 + c_3 + c_4)n=a + bn, where a\\coloneqq(c_1 + c_5) and b\\coloneqq(c_2 + c_3 + c_4). This is a linear function of the input size.\n\n\nSUM-MATRIX\n\nExercise 1.11 Analyze the running time of SUM-MATRIX.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe refer to the pseudocode of SUM-MATRIX for this analysis.\n\n\n\nLine\nCost\nTimes\n\n\n\n\n2\nc_1\n1\n\n\n3\nc_2\nn\n\n\n4\nc_3\nn\\times n\n\n\n5\nc_4\nn\\times n\n\n\n6\nc_5\nn\\times n\n\n\n8\nc_6\nn\n\n\n10\nc_7\n1\n\n\n\nSo, the running time T(n)=(c_1 + c_7) + (c_2 + c_6)n + (c_3 + c_4 + c_5)n^2=a + bn+cn^2, where a\\coloneqq(c_1 + c_7), b\\coloneqq(c_2 + c_6), and c\\coloneqq(c_3 + c_4 + c_5). This is a quadratic function of the input size.\n\n\n\n\n\nSEARCH-ARRAY\nFor this analysis, we first note the best case and the worst case inputs.\nThe best case is when the value x is the first element of the input array A. The search halts in the very first iteration. In this case, the running time is T(n)=a for some constant a.\nThe worst case occurs when the value x is the last element of the input array A or x does not belong to A. The search, in this case, loops through all the elements of the array. In this case, the running time is T(n)=a + bn for some constants a,b.\n\n\n\nTuring Machine taken from Markoš, Kelemen, 2004",
    "crumbs": [
      "Supervised",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "notes/svm.html",
    "href": "notes/svm.html",
    "title": "3  Support Vector Machines",
    "section": "",
    "text": "3.1 Introduction\nAs always, we ask the following questions as we encounter a new learning algorithm:",
    "crumbs": [
      "Supervised",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Support Vector Machines</span>"
    ]
  },
  {
    "objectID": "notes/svm.html#introduction",
    "href": "notes/svm.html#introduction",
    "title": "3  Support Vector Machines",
    "section": "",
    "text": "Classification or regression?\n\nClassification—default is binary\n\nSupervised or unsupervised?\n\nSupervised—training data are labeled\n\nParametric or non-parametric?\n\nParametric—it assumes a hypothesis class, namely hyperplanes",
    "crumbs": [
      "Supervised",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Support Vector Machines</span>"
    ]
  },
  {
    "objectID": "notes/svm.html#a-motivating-example",
    "href": "notes/svm.html#a-motivating-example",
    "title": "3  Support Vector Machines",
    "section": "3.2 A Motivating Example",
    "text": "3.2 A Motivating Example\nLet us ask the curious question: can we learn to predict party affiliation of a US national from their age and years of education?\n\nNumber of features: n=2.\nFeature space: \\mathcal{X} is a subset of \\mathbb{R}^2.\nTarget space: \\mathcal{Y}={RED, BLUE}.\n\n\n\n\n\npivots = Object({ x1: 20, y1: 1, x2: 70, y2: 10 });\ndata = generateData(18, 80, 0, 15, 20, separable ? \"linear\" : \"quad\");\ndraw(\n  data.map((d) =&gt; ({ x: d.x, y: d.y, z: d.z })),\n  {\n    x: \"x\",\n    y: \"y\",\n    xdomain: [18, 80],\n    ydomain: [0, 15]\n  }\n);\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nviewof showLine = Inputs.toggle({ value: false, label: \"Show line\" });\nviewof margin = Inputs.range([0, 5], {\n  value: 0,\n  step: 0.01,\n  label: \"Margin\"\n});\nviewof showRegions = Inputs.toggle({ value: false, label: \"Classified regions\" });\nviewof separable = Inputs.toggle({ value: true, label: \"Separable?\" });\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn the demo, we are trying to:\n\nseparate the separate our training labelled examples by a line (more generally a hyperplane).\nonce we have learned a separating hyperplane, an unseen test point can be classified based on which side of the hyperplane the point is.\n\n\n\n\n\n\n\nSummary\n\n\n\nIn summary, the SVM tries to learn from a sample a linear decision boundary that fully separates the training data—when possible.\n\n\n\nPros\n\nVery intuitive\nEfficient algorithms are available\n\nLinear Programming Problem\nPerceptron (Rosenblatt 1958)\n\nEasily generalized to a higher-dimensional feature space\n\ndecision boundary: line (n=2), plane (n=3), hyperplane (n\\geq3)\n\n\n\nRosenblatt, F. 1958. “The perceptron: A probabilistic model for information storage and organization in the brain.” Psychological Review 65 (6): 386–408. https://doi.org/10.1037/h0042519.\n\n\nCons\n\nThere are infinitely many separating hyperplanes\n\nmaximum-margin\n\nThe data may not be always linearly separable\n\nsoft-margin\nKernel methods",
    "crumbs": [
      "Supervised",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Support Vector Machines</span>"
    ]
  },
  {
    "objectID": "notes/svm.html#separable-case-hard-margin",
    "href": "notes/svm.html#separable-case-hard-margin",
    "title": "3  Support Vector Machines",
    "section": "3.3 Separable Case (Hard-Margin)",
    "text": "3.3 Separable Case (Hard-Margin)\n\nMathematical Formulation\nTraining Sample: S=\\{(\\pmb{x_1},y_1),\\ldots,(\\pmb{x_m},y_m)\\}\n\neach \\pmb{x_i} is an n\\times 1 feature vector\nsize m\nI.I.D.\n\\pmb{x}=(x_1,\\ldots,x_n) is a vector in \\mathbb{R}^n\ny\\in\\{-1, +1\\}\n\nObjective: Look for a hyperplane \\pmb{w}\\cdot\\pmb{x}+b=0 with the normal vector \\pmb{w} such that\n\nall sample points are on the correct side of the hyperplane, i.e., y_i(\\pmb{\\pmb{w}\\cdot\\pmb{x_i}}+b)\\geq0\\text{ for all }i\\in[m]\nthe margin (distance to the closed sample point) \\rho=\\min_{i\\in[m]}\\frac{|\\pmb{w}\\cdot\\pmb{x_i}+b|}{\\|\\pmb{w}\\|} is maximum\n\n\n\n\nThe hyperplane\n\n\n\n\nThe Solution\nThe good news is a unique solution hyperplane exists—so long as the sample points are linearly separable.\nIn that case, the solution \\pmb{w}^* is a linear combination of the training set vectors: \n\\pmb{w^*}=\\alpha_1\\pmb{x_1}+\\ldots+\\alpha_m\\pmb{x_m}.\n If the i-th training vector appears in the above linear combination (i.e., \\alpha_i\\neq0), then it’s called a support vector. Moreover, for any such support vector: \nb^*=y_i-\\sum_{j=1}^m\\alpha_j y_j (\\pmb{x}_j\\cdot\\pmb{x_i}).\n\nFor a test data point with feature vector \\pmb{x}, we classify using the following rule: \n\\pmb{x}\\mapsto\\text{sign}(\\pmb{w^*}\\cdot\\pmb{x}+b^*)\n=\\text{sgn}\\left(\\sum_{i=1}^m\\alpha_iy_i\\langle\\pmb{x_i},\\pmb{x}\\rangle+b^*\\right).\n\n\n\n\n\n\n\nSolving the Optimization Problem\n\n\n\n\n\nThe primal problem is to find a hyperplane (given by the normal vector \\pmb{w}\\in\\mathbb{R}^n and b\\in\\mathbb{R}) so that \n\\min_{\\pmb{w},b}\\frac{1}{2}\\|\\pmb{w}\\|^2\n subject to: y_i(\\pmb{w}\\cdot\\pmb{x_i}+b)\\geq1\\text{ for all }i\\in[m] This is a convex optimization problem with a unique solution.\nHowever, in order to get the optimal solution, we consider the dual problem. For more details Mohri, Rostamizadeh, and Talwalkar (2018).\n\n\n\n\n\n\n\n\n\nTip",
    "crumbs": [
      "Supervised",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Support Vector Machines</span>"
    ]
  },
  {
    "objectID": "notes/svm.html#non-separable-case-soft-margin",
    "href": "notes/svm.html#non-separable-case-soft-margin",
    "title": "3  Support Vector Machines",
    "section": "3.4 Non-Separable Case (Soft-Margin)",
    "text": "3.4 Non-Separable Case (Soft-Margin)\nIn real applications, the sample points are not separable. In that case, we allow for exceptions.\nWe would not mind a few exceptional sample points lying inside the maximum margin or even on the wrong side of the margin. However, the less number of exceptions, the better.\nWe toss a hyper-parameter C\\geq0 (known as the regularization parameter) in to our optimization.\n\n\n\n\n\n\nSolving with Slack Variables\n\n\n\n\n\nThe primal problem is to find a hyperplane (given by the normal vector \\pmb{w}\\in\\mathbb{R}^n and b\\in\\mathbb{R}) so that \n\\min_{\\pmb{w},b,\\pmb{\\xi}}\\frac{1}{2}\\|\\pmb{w}\\|^2 + C\\sum_{i=1}^m\\xi^2_i\n subject to: y_i(\\pmb{w}\\cdot\\pmb{x_i}+b)\\geq1-\\xi_i\\text{ for all }i\\in[m] Here, the \\pmb{\\xi}=(\\xi_1,\\ldots,\\xi_m) is called the slack.\nThis is a also convex optimization problem with a unique solution.\nHowever, in order to get the optimal solution, we consider the dual problem. For more details (Mohri, Rostamizadeh, and Talwalkar 2018, chap. 5).\n\n\n\n\nMohri, Mehryar, Afshin Rostamizadeh, and Ameet Talwalkar. 2018. Foundations of Machine Learning. 2nd ed. The MIT Press.\nConsequently the objective of the optimization becomes two-fold:\n\nmaximize the margin\nlimit the total amount of slack.\n\n\n\n\n\n\n\nAbout the C\n\n\n\nThe regularization parameter C controls the width of the margin. The smaller C gets, the wider the margin becomes.\nSome of the common choices are 0.001, 0.01, 0.1, 1.0, 100, etc.\n\n\n\n\n\n\n\n\nTip",
    "crumbs": [
      "Supervised",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Support Vector Machines</span>"
    ]
  },
  {
    "objectID": "notes/svm.html#kernel-method",
    "href": "notes/svm.html#kernel-method",
    "title": "3  Support Vector Machines",
    "section": "3.5 Kernel Method",
    "text": "3.5 Kernel Method",
    "crumbs": [
      "Supervised",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Support Vector Machines</span>"
    ]
  },
  {
    "objectID": "notes/svm.html#multi-class-classification",
    "href": "notes/svm.html#multi-class-classification",
    "title": "3  Support Vector Machines",
    "section": "3.6 Multi-class Classification",
    "text": "3.6 Multi-class Classification",
    "crumbs": [
      "Supervised",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Support Vector Machines</span>"
    ]
  },
  {
    "objectID": "notes/svm.html#appendix",
    "href": "notes/svm.html#appendix",
    "title": "3  Support Vector Machines",
    "section": "3.7 Appendix",
    "text": "3.7 Appendix\n\ncompute_c = function (points) {\n  return points.y1 - compute_m(points) * points.x1;\n}\ncompute_m = function (points) {\n  return (points.y1 - points.y2) / (points.x1 - points.x2);\n}\nupdate = function (svg, X, Y, zrange, strech, sep) {\n  const xdomain = X.domain();\n  const ydomain = Y.domain();\n  const m = compute_m(pivots);\n  const c = compute_c(pivots);\n\n  svg\n    .select(\"#line\")\n    .attr(\"stroke\", sep ? \"black\" : \"lightgray\")\n    .attr(\n      \"x1\",\n      m * xdomain[0] + c &gt;= ydomain[0] ? X(xdomain[0]) : X((ydomain[0] - c) / m)\n    )\n    .attr(\n      \"y1\",\n      m * xdomain[0] + c &gt;= ydomain[0] ? Y(m * xdomain[0] + c) : Y(ydomain[0])\n    )\n    .attr(\n      \"x2\",\n      m * xdomain[1] + c &gt;= ydomain[0] ? X(xdomain[1]) : X((ydomain[0] - c) / m)\n    )\n    .attr(\n      \"y2\",\n      m * xdomain[1] + c &gt;= ydomain[0] ? Y(m * xdomain[1] + c) : Y(ydomain[0])\n    );\n\n  svg\n    .select(\"#R1\")\n    .attr(\n      \"points\",\n      [\n        [X(xdomain[0]), Y(m * xdomain[0] + c)],\n        [X(xdomain[1]), Y(m * xdomain[1] + c)],\n        [X(xdomain[1]), Y(m * xdomain[1] + c + strech)],\n        [X(xdomain[0]), Y(m * xdomain[0] + c + strech)]\n      ].join(\",\")\n    )\n    .style(\"fill\", zrange[0])\n    .style(\"opacity\", 0.1);\n  svg\n    .select(\"#R2\")\n    .attr(\n      \"points\",\n      [\n        [X(xdomain[0]), Y(m * xdomain[0] + c)],\n        [X(xdomain[1]), Y(m * xdomain[1] + c)],\n        [X(xdomain[1]), Y(m * xdomain[1] + c - strech)],\n        [X(xdomain[0]), Y(m * xdomain[0] + c - strech)]\n      ].join(\",\")\n    )\n    .style(\"fill\", zrange[1])\n    .style(\"opacity\", 0.1);\n\n  svg\n    .select(\"#M1\")\n    .attr(\n      \"points\",\n      [\n        [X(xdomain[0]), Y(m * xdomain[0] + c)],\n        [X(xdomain[1]), Y(m * xdomain[1] + c)],\n        [X(xdomain[1]), Y(m * xdomain[1] + c + margin * Math.sqrt(1 + m * m))],\n        [X(xdomain[0]), Y(m * xdomain[0] + c + margin * Math.sqrt(1 + m * m))]\n      ].join(\",\")\n    )\n    .style(\"fill\", zrange[0])\n    .style(\"opacity\", 0.3);\n\n  svg\n    .select(\"#M2\")\n    .attr(\n      \"points\",\n      [\n        [X(xdomain[0]), Y(m * xdomain[0] + c)],\n        [X(xdomain[1]), Y(m * xdomain[1] + c)],\n        [X(xdomain[1]), Y(m * xdomain[1] + c - margin * Math.sqrt(1 + m * m))],\n        [X(xdomain[0]), Y(m * xdomain[0] + c - margin * Math.sqrt(1 + m * m))]\n      ].join(\",\")\n    )\n    .style(\"fill\", zrange[1])\n    .style(\"opacity\", 0.3);\n}\nseparates = function (data, x, y, z) {\n  const m = compute_m(pivots);\n  const c = compute_c(pivots);\n  if (data[0][y] &gt; m * data[0][x] + c)\n    return data.every(\n      (d) =&gt;\n        (d[z] == data[0][z] && d[y] &gt; m * d[x] + c) ||\n        (d[z] != data[0][z] && d[y] &lt; m * d[x] + c)\n    );\n  else if (data[0][y] &lt; m * data[0][x] + c)\n    return data.every(\n      (d) =&gt;\n        (d[z] == data[0][z] && d[y] &lt; m * d[x] + c) ||\n        (d[z] != data[0][z] && d[y] &gt; m * d[x] + c)\n    );\n  else return false;\n}\ndraw = function (data, args = {}) {\n  // Declare the chart dimensions and margins.\n  const width = args.width || 500;\n  const height = args.width || 400;\n  const marginTop = args.marginTop || 5;\n  const marginRight = args.marginRight || 20;\n  const marginBottom = args.marginBottom || 50;\n  const marginLeft = args.marginLeft || 40;\n  const x = args.x || \"x\";\n  const y = args.y || \"y\";\n  const z = args.z || \"z\";\n  const xdomain = args.xdomain || [0, d3.max(data, (d) =&gt; d[x])];\n  const ydomain = args.ydomain || [0, d3.max(data, (d) =&gt; d[y])];\n  const zdomain = args.zdomain || [0, 1];\n  const zrange = args.zrange || [\"red\", \"blue\"];\n  const m = compute_m(pivots);\n  const c = compute_c(pivots);\n\n  // Declare the x (horizontal position) scale.\n  const X = d3\n    .scaleLinear()\n    .domain(xdomain)\n    .range([marginLeft, width - marginRight]);\n\n  // Declare the y (vertical position) scale.\n  const Y = d3\n    .scaleLinear()\n    .domain(ydomain)\n    .range([height - marginBottom, marginTop]);\n\n  // Declare the fill axis\n  const Z = d3.scaleOrdinal().domain(zdomain).range(zrange);\n\n  // Create the SVG container.\n  const svg = d3.create(\"svg\").attr(\"width\", width).attr(\"height\", height);\n\n  // Add the x-axis.\n  svg\n    .append(\"g\")\n    .attr(\"transform\", `translate(0,${height - marginBottom})`)\n    .call(d3.axisBottom(X));\n  svg.append(\"text\")\n    .attr(\"text-anchor\", \"end\")\n    .attr(\"x\", width - 50)\n    .attr(\"y\", height - 20 )\n    .text(\"age (x1) →\");\n  svg.append(\"text\")\n    .attr(\"text-anchor\", \"end\")\n    .attr(\"transform\", \"rotate(-90)\")\n    .attr(\"y\",  13)\n    .attr(\"x\", -20)\n    .text(\"education (x2) →\")\n\n  // Add the y-axis.\n  svg\n    .append(\"g\")\n    .attr(\"transform\", `translate(${marginLeft},0)`)\n    .call(d3.axisLeft(Y));\n\n  const regions = svg.append(\"g\").attr(\"id\", \"regions\");\n  regions.append(\"polygon\").attr(\"id\", \"R1\");\n  regions.append(\"polygon\").attr(\"id\", \"R2\");\n\n  // Add the dots\n  svg\n    .append(\"g\")\n    .selectAll(\"dot\")\n    .data(data)\n    .enter()\n    .append(\"circle\")\n    .attr(\"cx\", (d) =&gt; X(d[x]))\n    .attr(\"cy\", (d) =&gt; Y(d[y]))\n    .attr(\"r\", 3)\n    .style(\"fill\", (d) =&gt; Z(d[z]));\n\n  // Show Margins\n  svg.append(\"g\").attr(\"id\", \"margins\");\n  regions.append(\"polygon\").attr(\"id\", \"M1\");\n  regions.append(\"polygon\").attr(\"id\", \"M2\");\n\n  // Add the line\n  const line = svg.append(\"g\");\n\n  line\n    .append(\"line\")\n    .attr(\"id\", \"line\")\n    .attr(\"stroke-width\", 3)\n    .attr(\"path-length\", 10);\n\n  update(svg, X, Y, zrange, Math.max(width, height), separates(data, x, y, z));\n\n  // Draw the pivot points\n  line\n    .append(\"circle\")\n    .attr(\"cx\", X(pivots.x1))\n    .attr(\"cy\", Y(pivots.y1))\n    .attr(\"r\", 7)\n    .style(\"fill\", \"white\")\n    .style(\"stroke\", \"lightgrey\")\n    .style(\"stroke-width\", 3)\n    .call(\n      d3.drag().on(\"drag\", function (event, d) {\n        pivots.x1 = X.invert(event.x);\n        pivots.y1 = Y.invert(event.y);\n\n        d3.select(this).attr(\"cx\", X(pivots.x1)).attr(\"cy\", Y(pivots.y1));\n        update(\n          svg,\n          X,\n          Y,\n          zrange,\n          Math.max(width, height),\n          separates(data, x, y, z)\n        );\n      })\n    );\n  line\n    .append(\"circle\")\n    .attr(\"cx\", X(pivots.x2))\n    .attr(\"cy\", Y(pivots.y2))\n    .attr(\"r\", 7)\n    .style(\"fill\", \"white\")\n    .style(\"stroke\", \"lightgrey\")\n    .style(\"stroke-width\", 3)\n    .call(\n      d3.drag().on(\"drag\", function (event, d) {\n        pivots.x2 = X.invert(event.x);\n        pivots.y2 = Y.invert(event.y);\n\n        d3.select(this).attr(\"cx\", X(pivots.x2)).attr(\"cy\", Y(pivots.y2));\n        update(\n          svg,\n          X,\n          Y,\n          zrange,\n          Math.max(width, height),\n          separates(data, x, y, z)\n        );\n      })\n    );\n\n  // Draw regions\n  if (showRegions) {\n    regions.attr(\"visibility\", \"visible\");\n    regions.attr(\"visibility\", \"visible\");\n  } else {\n    regions.attr(\"visibility\", \"hidden\");\n    regions.attr(\"visibility\", \"hidden\");\n  }\n\n  // Draw line\n  if (showLine) {\n    line.attr(\"visibility\", \"visible\");\n    line.attr(\"visibility\", \"visible\");\n  } else {\n    line.attr(\"visibility\", \"hidden\");\n    line.attr(\"visibility\", \"hidden\");\n  }\n\n  // Return the SVG element.\n  return svg.node();\n}\ngenerateData = function (xmin = -1, xmax = 1, ymin = -1, ymax = 1, n = 10, method = \"linear\") {\n  const randX = d3.randomUniform(xmin, xmax);\n  const randY = d3.randomUniform(ymin, ymax);\n  const pivots = {\n    x1: randX(),\n    x2: randX(),\n    y1: randY(),\n    y2: randY()\n  };\n  const a = d3.randomUniform(0, 20)();\n  const b = d3.randomUniform(0, 10)();\n\n  return d3.range(n).map((d) =&gt; {\n    const x = randX();\n    const y = randY();\n    let z;\n    if (method == \"linear\")\n      z = y - compute_m(pivots) * x - compute_c(pivots) &gt; 0 ? 0 : 1;\n    else if (method == \"quad\") {\n      z = (x - 50) * (x - 50) / (a * a) + (y - 8) * (y - 8) / (b * b) - 1 &gt; 0 ? 0 : 1;\n    }\n    return {\n      x: x,\n      y: y,\n      z: z\n    };\n  });\n}",
    "crumbs": [
      "Supervised",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Support Vector Machines</span>"
    ]
  },
  {
    "objectID": "appendix/math.html",
    "href": "appendix/math.html",
    "title": "Appendix A — Mathematical Foundations",
    "section": "",
    "text": "A.1 Sets\nThe following statements are True or False?",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Mathematical Foundations</span>"
    ]
  },
  {
    "objectID": "appendix/math.html#sets",
    "href": "appendix/math.html#sets",
    "title": "Appendix A — Mathematical Foundations",
    "section": "",
    "text": "Exercise A.1 \\mathbb{Q}\\subsetneq\\mathbb{R}?\n\n\nSolution. \n\nTrue\n\nNote that A\\subsetneq B denotes a proper subset. The set of reals \\mathbb{R} contains the rationals (e.g. numbers of the form p/q for integers p,q) and the irrationals (e.g. the numbers e,\\pi, etc.).\n\n\nExercise A.2 \\mathbb{Z}\\subset\\mathbb{N}\n\n\nSolution. \n\nFalse\n\nNote that A\\subset B denotes a subset, without the restriction of being proper. So, A\\subset B implies that either A=B or A is a proper subset of B. Since \\mathbb{Z}, the set of integers, has all the naturals (\\mathbb{N}) and more (e.g. 0,-1,-2), therefore neither of the above is true.\n\n\nExercise A.3 A\\cap\\emptyset=\\emptyset\n\n\nSolution. \n\nTrue\n\n\n\nExercise A.4 A\\cup B\\subsetneq B\n\n\nSolution. \n\nFalse\n\n\n\nExercise A.5 For two finite sets A and B, we have |A\\cup B|=|A|+|B|.\n\n\nSolution. \n\nFalse (i.e., not true in general)\n\nTo show that the statement is false, we need to pick just one particular example of A,B such that the relation holds false!\nWe pick A=\\{1, 2, 3\\} and B=\\{1, 2\\}.\n\n\nExercise A.6 A\\cap(B\\cup C)=(A\\cap B)\\cup(A\\cap C). Draw to illustrate.\n\n\nSolution. \n\nTrue\n\nThis property is known as the distributive property of set intersection.\n\n\nExercise A.7 (A\\cup B)^c=A^c\\cap B^c.\n\n\nSolution. \n\nTrue\n\nThis formula is known as DeMorgan’s Law of set complementation.\n\n\nExercise A.8 For a finite set A, its power set, denoted by \\mathcal{P}(A), has 2^{|A|} many elements.\n\n\nSolution. \n\nTrue",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Mathematical Foundations</span>"
    ]
  },
  {
    "objectID": "appendix/math.html#functions",
    "href": "appendix/math.html#functions",
    "title": "Appendix A — Mathematical Foundations",
    "section": "A.2 Functions",
    "text": "A.2 Functions\n\nExercise A.9 Draw the graphs of \\log_{e}(x), the natural logarithm function.\n\n\nExercise A.10 How is the above graph different from the graph of \\log_{2}(x)?\n\n\nExercise A.11 Draw the graph of x^2, \\sqrt{x}, and x in the same plot.\n\n\nExercise A.12 Can you find an m such that the line y=mx stays above the graph of \\sqrt{x} for any large positive x?\n\n\nExercise A.13 Can you find an m such that the line y=mx stays above the graph of x^2 for any large positive x?",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Mathematical Foundations</span>"
    ]
  },
  {
    "objectID": "appendix/math.html#summation-notation-series",
    "href": "appendix/math.html#summation-notation-series",
    "title": "Appendix A — Mathematical Foundations",
    "section": "A.3 Summation Notation, Series",
    "text": "A.3 Summation Notation, Series\n\nSum of natural numbers: \\sum_{k=1}^n k=\\frac{n(n+1)}{2}. \\tag{A.1}\nFinite geometric series: \\sum_{k=0}^n x^k=\\frac{x^{n+1}-1}{x-1}. \\tag{A.2}\nInfinite geometric series for |x|&lt;1: \\sum_{k=0}^\\infty x^k=\\frac{1}{1-x}. \\tag{A.3}\nFor |x|&lt;1: \\sum_{k=0}^\\infty kx^k=\\frac{x}{(1-x)^2}. \\tag{A.4}",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Mathematical Foundations</span>"
    ]
  },
  {
    "objectID": "appendix/math.html#mathematical-induction",
    "href": "appendix/math.html#mathematical-induction",
    "title": "Appendix A — Mathematical Foundations",
    "section": "A.4 Mathematical Induction",
    "text": "A.4 Mathematical Induction\n\nExercise A.14 What is mathematical induction?\n\n\nExercise A.15 Prove the first identity above using mathematical induction.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Mathematical Foundations</span>"
    ]
  },
  {
    "objectID": "appendix/linal.html",
    "href": "appendix/linal.html",
    "title": "Appendix B — Linear Algebra",
    "section": "",
    "text": "Unable to display PDF file. Download instead.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Linear Algebra</span>"
    ]
  }
]