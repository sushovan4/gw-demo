[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning",
    "section": "",
    "text": "Welcome",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#who-is-the-instructor",
    "href": "index.html#who-is-the-instructor",
    "title": "Machine Learning",
    "section": "Who is the instructor?",
    "text": "Who is the instructor?\nName: Dr. Sushovan Majhi\nEmail: s.majhi@gwu.edu\nHomepage: smajhi.com",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "notes/svm.html",
    "href": "notes/svm.html",
    "title": "3  Support Vector Machines",
    "section": "",
    "text": "3.1 Introduction\nAs always, we ask the following questions as we encounter a new learning algorithm:",
    "crumbs": [
      "Classical Methods",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Support Vector Machines</span>"
    ]
  },
  {
    "objectID": "notes/svm.html#introduction",
    "href": "notes/svm.html#introduction",
    "title": "3  Support Vector Machines",
    "section": "",
    "text": "Classification or regression?\n\nClassification—default is binary\n\nSupervised or unsupervised?\n\nSupervised—training data are labeled\n\nParametric or non-parametric?\n\nParametric—it assumes a hypothesis class, namely hyperplanes",
    "crumbs": [
      "Classical Methods",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Support Vector Machines</span>"
    ]
  },
  {
    "objectID": "notes/svm.html#a-motivating-example",
    "href": "notes/svm.html#a-motivating-example",
    "title": "3  Support Vector Machines",
    "section": "3.2 A Motivating Example",
    "text": "3.2 A Motivating Example\nLet us ask the curious question: can we learn to predict party affiliation of a US national from their age and years of education?\n\nNumber of features: n=2.\nFeature space: \\mathcal{X} is a subset of \\mathbb{R}^2.\nTarget space: \\mathcal{Y}={RED, BLUE}.\n\n\n\n\n\npivots = Object({ x1: 20, y1: 1, x2: 70, y2: 10 });\ndata = generateData(18, 80, 0, 15, 20, separable ? \"linear\" : \"quad\");\ndraw(\n  data.map((d) =&gt; ({ x: d.x, y: d.y, z: d.z })),\n  {\n    x: \"x\",\n    y: \"y\",\n    xdomain: [18, 80],\n    ydomain: [0, 15]\n  }\n);\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nviewof showLine = Inputs.toggle({ value: false, label: \"Show line\" });\nviewof margin = Inputs.range([0, 5], {\n  value: 0,\n  step: 0.01,\n  label: \"Margin\"\n});\nviewof showRegions = Inputs.toggle({ value: false, label: \"Classified regions\" });\nviewof separable = Inputs.toggle({ value: true, label: \"Separable?\" });\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn the demo, we are trying to:\n\nseparate the separate our training examples by a line.\nonce we have learned a separating line, an unseen test point can be classified based on which side of the hyperplane the point is.\n\n\n\n\n\n\n\nSummary\n\n\n\nIn summary, the SVM tries to learn from a sample a linear decision boundary that fully separates the training data.\n\n\n\nPros\n\nVery intuitive\nEfficient algorithms are available\n\nLinear Programming Problem\nPerceptron (Rosenblatt 1958)\n\nEasily generalized to a higher-dimensional feature space\n\ndecision boundary: line (n=2), plane (n=3), hyperplane (n\\geq3)\n\n\n\n\nCons\n\nThere are infinitely many separating hyperplanes\n\nmaximum-margin\n\nThe data may not be always linearly separable\n\nsoft-margin\nKernel methods",
    "crumbs": [
      "Classical Methods",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Support Vector Machines</span>"
    ]
  },
  {
    "objectID": "notes/svm.html#the-separable-case-hard-margin",
    "href": "notes/svm.html#the-separable-case-hard-margin",
    "title": "3  Support Vector Machines",
    "section": "3.3 The Separable Case (Hard-Margin)",
    "text": "3.3 The Separable Case (Hard-Margin)\nLet’s assume that the data points are linearly separable. Out of the infinitely many separating lines, we find the one with the largest margin.\n\nMathematical Formulation\n\nFeature Space:\n\n\\mathcal{X}\\subset\\mathbb{R}^n is an n-dimensional space.\nfeature vector \\pmb{x} is an n\\times 1 vector\n\n\n\nTarget Space:\n\n\\mathcal{Y}=\\{-1, 1\\}\n\n\n\nTraining Sample:\n\nsize m\nS=\\{(\\pmb{x_1},y_1),\\ldots,(\\pmb{x_m},y_m)\\}\n\neach x_i\\in\\mathcal{X}\neach y_i\\in\\mathcal{Y}\n\nI.I.D. (why?)\n\n\n\nObjective\n\nFind a hyperplane \\pmb{w}\\cdot\\pmb{x}+b=0 such that\nall sample points are on the correct side of the hyperplane, i.e., y_i(\\pmb{\\pmb{w}\\cdot\\pmb{x_i}}+b)\\geq0\\text{ for all }i\\in[1,m]\nthe margin (distance to the closed sample point) \\rho=\\min_{i\\in[1, m]}\\frac{|\\pmb{w}\\cdot\\pmb{x_i}+b|}{\\|\\pmb{w}\\|} is maximum\n\n\nThe good news is a unique solution hyperplane exists—so long as the sample points are linearly separable.\n\n\n\n\n\n\nSolving the Optimization Problem\n\n\n\n\n\nThe primal problem can be stated as \n\\min_{\\pmb{w},b}\\frac{1}{2}\\|\\pmb{w}\\|^2\n subject to: y_i(\\pmb{w}\\cdot\\pmb{x_i}+b)\\geq1\\text{ for all }i\\in[1,m] This is a convex optimization problem with a unique solution (\\pmb{w}^*,b*).\nHence, the problem can be solved using quadratic programming (QP).\nMoreover, the normal vector \\pmb{w}^* is a linear combination of the training feature vectors: \n\\pmb{w^*}=\\alpha_1\\pmb{x_1}+\\ldots+\\alpha_m\\pmb{x_m}.\n If the i-th training vector appears in the above linear combination (i.e., \\alpha_i\\neq0), then it’s called a support vector.\n\n\n\n\n\n\n\n\n\nDecision Rule\n\n\n\nFor an unseen test data-point with feature vector \\pmb{x}, we classify using the following rule: \n\\pmb{x}\\mapsto\\text{sign}(\\pmb{w}^*\\cdot\\pmb{x}+b^*)\n=\\text{sgn}\\left(\\sum_{i=1}^m\\alpha_iy_i\\pmb{x_i}\\cdot\\pmb{x}+b^*\\right).\n\n\n\n\n\n\n\n\n\nCode",
    "crumbs": [
      "Classical Methods",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Support Vector Machines</span>"
    ]
  },
  {
    "objectID": "notes/svm.html#the-non-separable-case-soft-margin",
    "href": "notes/svm.html#the-non-separable-case-soft-margin",
    "title": "3  Support Vector Machines",
    "section": "3.4 The Non-Separable Case (Soft-Margin)",
    "text": "3.4 The Non-Separable Case (Soft-Margin)\nIn real applications, the sample points are never separable. In that case, we allow for exceptions.\nWe would not mind a few exceptional sample points lying inside the maximum margin or even on the wrong side of the margin. However, the less number of exceptions, the better.\nWe toss a hyper-parameter C\\geq0 (known as the regularization parameter) in to our optimization.\n\n\n\n\n\n\nSolving with Slack Variables\n\n\n\n\n\nThe primal problem is to find a hyperplane (given by the normal vector \\pmb{w}\\in\\mathbb{R}^n and b\\in\\mathbb{R}) so that \n\\min_{\\pmb{w},b,\\pmb{\\xi}}\\left(\\frac{1}{2}\\|\\pmb{w}\\|^2 + C\\sum_{i=1}^m\\xi^2_i\\right)\n subject to: y_i(\\pmb{w}\\cdot\\pmb{x_i}+b)\\geq1-\\xi_i\\text{ for all }i\\in[m] Here, the \\pmb{\\xi}=(\\xi_1,\\ldots,\\xi_m) is called the slack.\nThis is a also convex optimization problem with a unique solution.\nHowever, in order to get the optimal solution, we consider the dual problem. For more details (Mohri, Rostamizadeh, and Talwalkar 2018, chap. 5).\n\n\n\nConsequently the objective of the optimization becomes two-fold:\n\nmaximize the margin\nlimit the total amount of slack.\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\n\n\n\n\n\nChoosing C\nSome of the common choices are 0.001, 0.01, 0.1, 1.0, 100, etc. However, we usually use cross-validation to choose the best value for C.\n\n\n\n\n\n\nCode",
    "crumbs": [
      "Classical Methods",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Support Vector Machines</span>"
    ]
  },
  {
    "objectID": "notes/svm.html#non-linear-boundary",
    "href": "notes/svm.html#non-linear-boundary",
    "title": "3  Support Vector Machines",
    "section": "3.5 Non-Linear Boundary",
    "text": "3.5 Non-Linear Boundary\nFor datasets, the inherit decision boundary is non-linear.\n\nCan our SVM be extended to handle such cases?\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\n\n\n\n\n\nKernel Methods\nA kernel K:\\mathcal{X}\\times\\mathcal{X}\\to\\mathbb{R}^+ mimics the concept of inner-product, but for a more general Hilbert Space. Some of the good (for optimization) kernels are:\n\nLinear Kernel: hyperplane separation or usual inner-product K(\\pmb{x}, \\pmb{x}')=\\langle\\pmb{x}, \\pmb{x}'\\rangle\nRadial Basis Function (RBF): K(\\pmb{x}, \\pmb{x}')=\\exp{\\left(-\\frac{\\|\\pmb{x}-\\pmb{x}'\\|^2}{2\\sigma^2}\\right)}.\n\n\n\n\n\n\n\nCode",
    "crumbs": [
      "Classical Methods",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Support Vector Machines</span>"
    ]
  },
  {
    "objectID": "notes/svm.html#digit-recognition-using-svm",
    "href": "notes/svm.html#digit-recognition-using-svm",
    "title": "3  Support Vector Machines",
    "section": "3.6 Digit Recognition using SVM",
    "text": "3.6 Digit Recognition using SVM\n\n\n\n\n\n\nCode",
    "crumbs": [
      "Classical Methods",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Support Vector Machines</span>"
    ]
  },
  {
    "objectID": "notes/svm.html#conclusion",
    "href": "notes/svm.html#conclusion",
    "title": "3  Support Vector Machines",
    "section": "3.7 Conclusion",
    "text": "3.7 Conclusion\n\nTheoretically well-understood.\nExtremely promising in applications.\nWorks well for balanced data. If not balanced:\n\nresample\nuse class weights\n\nPrimarily a binary classifier. However, multi-class classification can be done using:\n\none-vs-others\none-vs-one\nRead more on Medium",
    "crumbs": [
      "Classical Methods",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Support Vector Machines</span>"
    ]
  },
  {
    "objectID": "notes/svm.html#resources",
    "href": "notes/svm.html#resources",
    "title": "3  Support Vector Machines",
    "section": "3.8 Resources",
    "text": "3.8 Resources\n\nBooks:\n\nIntroduction to Statistical Learning, Chapter 9.\n\nCode:\n\nsvm.ipynb\nsvm_plot.py\n\nHypothesis note-taking tool\n\n\nReferences\n\ncompute_c = function (points) {\n  return points.y1 - compute_m(points) * points.x1;\n}\ncompute_m = function (points) {\n  return (points.y1 - points.y2) / (points.x1 - points.x2);\n}\nupdate = function (svg, X, Y, zrange, strech, sep) {\n  const xdomain = X.domain();\n  const ydomain = Y.domain();\n  const m = compute_m(pivots);\n  const c = compute_c(pivots);\n\n  svg\n    .select(\"#line\")\n    .attr(\"stroke\", sep ? \"black\" : \"lightgray\")\n    .attr(\n      \"x1\",\n      m * xdomain[0] + c &gt;= ydomain[0] ? X(xdomain[0]) : X((ydomain[0] - c) / m)\n    )\n    .attr(\n      \"y1\",\n      m * xdomain[0] + c &gt;= ydomain[0] ? Y(m * xdomain[0] + c) : Y(ydomain[0])\n    )\n    .attr(\n      \"x2\",\n      m * xdomain[1] + c &gt;= ydomain[0] ? X(xdomain[1]) : X((ydomain[0] - c) / m)\n    )\n    .attr(\n      \"y2\",\n      m * xdomain[1] + c &gt;= ydomain[0] ? Y(m * xdomain[1] + c) : Y(ydomain[0])\n    );\n\n  svg\n    .select(\"#R1\")\n    .attr(\n      \"points\",\n      [\n        [X(xdomain[0]), Y(m * xdomain[0] + c)],\n        [X(xdomain[1]), Y(m * xdomain[1] + c)],\n        [X(xdomain[1]), Y(m * xdomain[1] + c + strech)],\n        [X(xdomain[0]), Y(m * xdomain[0] + c + strech)]\n      ].join(\",\")\n    )\n    .style(\"fill\", zrange[0])\n    .style(\"opacity\", 0.1);\n  svg\n    .select(\"#R2\")\n    .attr(\n      \"points\",\n      [\n        [X(xdomain[0]), Y(m * xdomain[0] + c)],\n        [X(xdomain[1]), Y(m * xdomain[1] + c)],\n        [X(xdomain[1]), Y(m * xdomain[1] + c - strech)],\n        [X(xdomain[0]), Y(m * xdomain[0] + c - strech)]\n      ].join(\",\")\n    )\n    .style(\"fill\", zrange[1])\n    .style(\"opacity\", 0.1);\n\n  svg\n    .select(\"#M1\")\n    .attr(\n      \"points\",\n      [\n        [X(xdomain[0]), Y(m * xdomain[0] + c)],\n        [X(xdomain[1]), Y(m * xdomain[1] + c)],\n        [X(xdomain[1]), Y(m * xdomain[1] + c + margin * Math.sqrt(1 + m * m))],\n        [X(xdomain[0]), Y(m * xdomain[0] + c + margin * Math.sqrt(1 + m * m))]\n      ].join(\",\")\n    )\n    .style(\"fill\", zrange[0])\n    .style(\"opacity\", 0.3);\n\n  svg\n    .select(\"#M2\")\n    .attr(\n      \"points\",\n      [\n        [X(xdomain[0]), Y(m * xdomain[0] + c)],\n        [X(xdomain[1]), Y(m * xdomain[1] + c)],\n        [X(xdomain[1]), Y(m * xdomain[1] + c - margin * Math.sqrt(1 + m * m))],\n        [X(xdomain[0]), Y(m * xdomain[0] + c - margin * Math.sqrt(1 + m * m))]\n      ].join(\",\")\n    )\n    .style(\"fill\", zrange[1])\n    .style(\"opacity\", 0.3);\n}\nseparates = function (data, x, y, z) {\n  const m = compute_m(pivots);\n  const c = compute_c(pivots);\n  if (data[0][y] &gt; m * data[0][x] + c)\n    return data.every(\n      (d) =&gt;\n        (d[z] == data[0][z] && d[y] &gt; m * d[x] + c) ||\n        (d[z] != data[0][z] && d[y] &lt; m * d[x] + c)\n    );\n  else if (data[0][y] &lt; m * data[0][x] + c)\n    return data.every(\n      (d) =&gt;\n        (d[z] == data[0][z] && d[y] &lt; m * d[x] + c) ||\n        (d[z] != data[0][z] && d[y] &gt; m * d[x] + c)\n    );\n  else return false;\n}\ndraw = function (data, args = {}) {\n  // Declare the chart dimensions and margins.\n  const width = args.width || 600;\n  const height = args.width || 400;\n  const marginTop = args.marginTop || 5;\n  const marginRight = args.marginRight || 20;\n  const marginBottom = args.marginBottom || 50;\n  const marginLeft = args.marginLeft || 40;\n  const x = args.x || \"x\";\n  const y = args.y || \"y\";\n  const z = args.z || \"z\";\n  const xdomain = args.xdomain || [0, d3.max(data, (d) =&gt; d[x])];\n  const ydomain = args.ydomain || [0, d3.max(data, (d) =&gt; d[y])];\n  const zdomain = args.zdomain || [0, 1];\n  const zrange = args.zrange || [\"red\", \"blue\"];\n  const m = compute_m(pivots);\n  const c = compute_c(pivots);\n\n  // Declare the x (horizontal position) scale.\n  const X = d3\n    .scaleLinear()\n    .domain(xdomain)\n    .range([marginLeft, width - marginRight]);\n\n  // Declare the y (vertical position) scale.\n  const Y = d3\n    .scaleLinear()\n    .domain(ydomain)\n    .range([height - marginBottom, marginTop]);\n\n  // Declare the fill axis\n  const Z = d3.scaleOrdinal().domain(zdomain).range(zrange);\n\n  // Create the SVG container.\n  const svg = d3.create(\"svg\").attr(\"width\", width).attr(\"height\", height);\n\n  // Add the x-axis.\n  svg\n    .append(\"g\")\n    .attr(\"transform\", `translate(0,${height - marginBottom})`)\n    .call(d3.axisBottom(X));\n  svg.append(\"text\")\n    .attr(\"text-anchor\", \"end\")\n    .attr(\"x\", width - 50)\n    .attr(\"y\", height - 20 )\n    .text(\"age (x1) →\");\n  svg.append(\"text\")\n    .attr(\"text-anchor\", \"end\")\n    .attr(\"transform\", \"rotate(-90)\")\n    .attr(\"y\",  13)\n    .attr(\"x\", -20)\n    .text(\"education (x2) →\")\n\n  // Add the y-axis.\n  svg\n    .append(\"g\")\n    .attr(\"transform\", `translate(${marginLeft},0)`)\n    .call(d3.axisLeft(Y));\n\n  const regions = svg.append(\"g\").attr(\"id\", \"regions\");\n  regions.append(\"polygon\").attr(\"id\", \"R1\");\n  regions.append(\"polygon\").attr(\"id\", \"R2\");\n\n  // Add the dots\n  svg\n    .append(\"g\")\n    .selectAll(\"dot\")\n    .data(data)\n    .enter()\n    .append(\"circle\")\n    .attr(\"cx\", (d) =&gt; X(d[x]))\n    .attr(\"cy\", (d) =&gt; Y(d[y]))\n    .attr(\"r\", 3)\n    .style(\"fill\", (d) =&gt; Z(d[z]));\n\n  // Show Margins\n  svg.append(\"g\").attr(\"id\", \"margins\");\n  regions.append(\"polygon\").attr(\"id\", \"M1\");\n  regions.append(\"polygon\").attr(\"id\", \"M2\");\n\n  // Add the line\n  const line = svg.append(\"g\");\n\n  line\n    .append(\"line\")\n    .attr(\"id\", \"line\")\n    .attr(\"stroke-width\", 3)\n    .attr(\"path-length\", 10);\n\n  update(svg, X, Y, zrange, Math.max(width, height), separates(data, x, y, z));\n\n  // Draw the pivot points\n  line\n    .append(\"circle\")\n    .attr(\"cx\", X(pivots.x1))\n    .attr(\"cy\", Y(pivots.y1))\n    .attr(\"r\", 7)\n    .style(\"fill\", \"white\")\n    .style(\"stroke\", \"lightgrey\")\n    .style(\"stroke-width\", 3)\n    .call(\n      d3.drag().on(\"drag\", function (event, d) {\n        pivots.x1 = X.invert(event.x);\n        pivots.y1 = Y.invert(event.y);\n\n        d3.select(this).attr(\"cx\", X(pivots.x1)).attr(\"cy\", Y(pivots.y1));\n        update(\n          svg,\n          X,\n          Y,\n          zrange,\n          Math.max(width, height),\n          separates(data, x, y, z)\n        );\n      })\n    );\n  line\n    .append(\"circle\")\n    .attr(\"cx\", X(pivots.x2))\n    .attr(\"cy\", Y(pivots.y2))\n    .attr(\"r\", 7)\n    .style(\"fill\", \"white\")\n    .style(\"stroke\", \"lightgrey\")\n    .style(\"stroke-width\", 3)\n    .call(\n      d3.drag().on(\"drag\", function (event, d) {\n        pivots.x2 = X.invert(event.x);\n        pivots.y2 = Y.invert(event.y);\n\n        d3.select(this).attr(\"cx\", X(pivots.x2)).attr(\"cy\", Y(pivots.y2));\n        update(\n          svg,\n          X,\n          Y,\n          zrange,\n          Math.max(width, height),\n          separates(data, x, y, z)\n        );\n      })\n    );\n\n  // Draw regions\n  if (showRegions) {\n    regions.attr(\"visibility\", \"visible\");\n    regions.attr(\"visibility\", \"visible\");\n  } else {\n    regions.attr(\"visibility\", \"hidden\");\n    regions.attr(\"visibility\", \"hidden\");\n  }\n\n  // Draw line\n  if (showLine) {\n    line.attr(\"visibility\", \"visible\");\n    line.attr(\"visibility\", \"visible\");\n  } else {\n    line.attr(\"visibility\", \"hidden\");\n    line.attr(\"visibility\", \"hidden\");\n  }\n\n  // Return the SVG element.\n  return svg.node();\n}\ngenerateData = function (xmin = -1, xmax = 1, ymin = -1, ymax = 1, n = 10, method = \"linear\") {\n  const randX = d3.randomUniform(xmin, xmax);\n  const randY = d3.randomUniform(ymin, ymax);\n  const pivots = {\n    x1: randX(),\n    x2: randX(),\n    y1: randY(),\n    y2: randY()\n  };\n  const a = d3.randomUniform(0, 20)();\n  const b = d3.randomUniform(0, 10)();\n\n  return d3.range(n).map((d) =&gt; {\n    const x = randX();\n    const y = randY();\n    let z;\n    if (method == \"linear\")\n      z = y - compute_m(pivots) * x - compute_c(pivots) &gt; 0 ? 0 : 1;\n    else if (method == \"quad\") {\n      z = (x - 50) * (x - 50) / (a * a) + (y - 8) * (y - 8) / (b * b) - 1 &gt; 0 ? 0 : 1;\n    }\n    return {\n      x: x,\n      y: y,\n      z: z\n    };\n  });\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCormen, Thomas H., Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein. 2009. Introduction to Algorithms, Third Edition. 3rd ed. The MIT Press.\n\n\nMohri, Mehryar, Afshin Rostamizadeh, and Ameet Talwalkar. 2018. Foundations of Machine Learning. 2nd ed. The MIT Press.\n\n\nRosenblatt, F. 1958. “The perceptron: A probabilistic model for information storage and organization in the brain.” Psychological Review 65 (6): 386–408. https://doi.org/10.1037/h0042519.",
    "crumbs": [
      "Classical Methods",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Support Vector Machines</span>"
    ]
  },
  {
    "objectID": "appendix/math.html",
    "href": "appendix/math.html",
    "title": "Appendix A — Mathematical Foundations",
    "section": "",
    "text": "A.1 Sets\nThe following statements are True or False?",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Mathematical Foundations</span>"
    ]
  },
  {
    "objectID": "appendix/math.html#sets",
    "href": "appendix/math.html#sets",
    "title": "Appendix A — Mathematical Foundations",
    "section": "",
    "text": "Exercise A.1 \\mathbb{Q}\\subsetneq\\mathbb{R}?\n\n\nSolution. \n\nTrue\n\nNote that A\\subsetneq B denotes a proper subset. The set of reals \\mathbb{R} contains the rationals (e.g. numbers of the form p/q for integers p,q) and the irrationals (e.g. the numbers e,\\pi, etc.).\n\n\nExercise A.2 \\mathbb{Z}\\subset\\mathbb{N}\n\n\nSolution. \n\nFalse\n\nNote that A\\subset B denotes a subset, without the restriction of being proper. So, A\\subset B implies that either A=B or A is a proper subset of B. Since \\mathbb{Z}, the set of integers, has all the naturals (\\mathbb{N}) and more (e.g. 0,-1,-2), therefore neither of the above is true.\n\n\nExercise A.3 A\\cap\\emptyset=\\emptyset\n\n\nSolution. \n\nTrue\n\n\n\nExercise A.4 A\\cup B\\subsetneq B\n\n\nSolution. \n\nFalse\n\n\n\nExercise A.5 For two finite sets A and B, we have |A\\cup B|=|A|+|B|.\n\n\nSolution. \n\nFalse (i.e., not true in general)\n\nTo show that the statement is false, we need to pick just one particular example of A,B such that the relation holds false!\nWe pick A=\\{1, 2, 3\\} and B=\\{1, 2\\}.\n\n\nExercise A.6 A\\cap(B\\cup C)=(A\\cap B)\\cup(A\\cap C). Draw to illustrate.\n\n\nSolution. \n\nTrue\n\nThis property is known as the distributive property of set intersection.\n\n\nExercise A.7 (A\\cup B)^c=A^c\\cap B^c.\n\n\nSolution. \n\nTrue\n\nThis formula is known as DeMorgan’s Law of set complementation.\n\n\nExercise A.8 For a finite set A, its power set, denoted by \\mathcal{P}(A), has 2^{|A|} many elements.\n\n\nSolution. \n\nTrue",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Mathematical Foundations</span>"
    ]
  },
  {
    "objectID": "appendix/math.html#functions",
    "href": "appendix/math.html#functions",
    "title": "Appendix A — Mathematical Foundations",
    "section": "A.2 Functions",
    "text": "A.2 Functions\n\nExercise A.9 Draw the graphs of \\log_{e}(x), the natural logarithm function.\n\n\nExercise A.10 How is the above graph different from the graph of \\log_{2}(x)?\n\n\nExercise A.11 Draw the graph of x^2, \\sqrt{x}, and x in the same plot.\n\n\nExercise A.12 Can you find an m such that the line y=mx stays above the graph of \\sqrt{x} for any large positive x?\n\n\nExercise A.13 Can you find an m such that the line y=mx stays above the graph of x^2 for any large positive x?",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Mathematical Foundations</span>"
    ]
  },
  {
    "objectID": "appendix/math.html#summation-notation-series",
    "href": "appendix/math.html#summation-notation-series",
    "title": "Appendix A — Mathematical Foundations",
    "section": "A.3 Summation Notation, Series",
    "text": "A.3 Summation Notation, Series\n\nSum of natural numbers: \\sum_{k=1}^n k=\\frac{n(n+1)}{2}. \\tag{A.1}\nFinite geometric series: \\sum_{k=0}^n x^k=\\frac{x^{n+1}-1}{x-1}. \\tag{A.2}\nInfinite geometric series for |x|&lt;1: \\sum_{k=0}^\\infty x^k=\\frac{1}{1-x}. \\tag{A.3}\nFor |x|&lt;1: \\sum_{k=0}^\\infty kx^k=\\frac{x}{(1-x)^2}. \\tag{A.4}",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Mathematical Foundations</span>"
    ]
  },
  {
    "objectID": "appendix/math.html#mathematical-induction",
    "href": "appendix/math.html#mathematical-induction",
    "title": "Appendix A — Mathematical Foundations",
    "section": "A.4 Mathematical Induction",
    "text": "A.4 Mathematical Induction\n\nExercise A.14 What is mathematical induction?\n\n\nExercise A.15 Prove the first identity above using mathematical induction.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Mathematical Foundations</span>"
    ]
  },
  {
    "objectID": "appendix/linal.html",
    "href": "appendix/linal.html",
    "title": "Appendix B — Linear Algebra",
    "section": "",
    "text": "Unable to display PDF file. Download instead.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Linear Algebra</span>"
    ]
  }
]