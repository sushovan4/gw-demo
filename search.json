[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome",
    "section": "",
    "text": "Welcome"
  },
  {
    "objectID": "notes/introduction.html",
    "href": "notes/introduction.html",
    "title": "1 Support Vector Machines (SVM)",
    "section": "",
    "text": "“Computers are to computer science what telescopes are to astronomy.”\n– E. Dijkstra",
    "crumbs": [
      "Lecture Notes",
      "Support Vector Machines (SVM)"
    ]
  },
  {
    "objectID": "notes/introduction.html#maximum-margin-classifier",
    "href": "notes/introduction.html#maximum-margin-classifier",
    "title": "1 Support Vector Machines (SVM)",
    "section": "1.1 Maximum-Margin Classifier",
    "text": "1.1 Maximum-Margin Classifier\n\nA Motivating Example\nLet us ask the curious question: can we learn to predict party affiliation of a US national from their age and years of education?\n\nNumber of features: n=2.\nFeature space: \\mathcal{X} is a subset of \\mathbb{R}^2.\nTarget space: \\mathcal{Y}={RED, BLUE}.\n\n\n\n\n\ndraw(\n  data1.map((d) =&gt; ({ x: d.x, y: d.y, z: d.z })),\n  {\n    x: \"x\",\n    y: \"y\",\n    xdomain: [-6, 6],\n    ydomain: [-6, 6]\n  }\n);\n\n\n\n\n\n\n\n\n\nviewof margin = Inputs.range([0, 5], {\n  value: 0,\n  step: 0.01,\n  label: \"Margin\"\n});\nviewof showLine = Inputs.toggle({ value: false, label: \"Show line\" });\nviewof showRegions = Inputs.toggle({ value: false, label: \"Classified regions\" });\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn the demo, we are trying to:\n\nseparate the separate our training labelled examples by a line (more generally a hyperplane).\nonce we have learned a separating hyperplane, an unseen test point can be classified based on which side of the hyperplane the point is.\n\n\n\n\n\n\n\nSummary\n\n\n\nIn summary, the SVM tries to learn from a sample a linear decision boundary that fully separates the training data—when possible.\n\n\n\n\nPros\n\nVery intuitive\nEfficient algorithms are available\n\nLinear Programming Problem\nPerceptron (Rosenblatt 1958)\n\nEasily generalized to a higher-dimensional feature space\n\ndecision boundary: line (n=2), plane (n=3), hyperplane (n\\geq3)\n\n\n\n\nCons\n\nThere are infinitely many separating hyperplanes\n\nmaximum-margin\n\nThe data may not be always linearly separable\n\nsoft-margin\nKernel methods",
    "crumbs": [
      "Lecture Notes",
      "Support Vector Machines (SVM)"
    ]
  },
  {
    "objectID": "notes/introduction.html#separable-case-hard-margin",
    "href": "notes/introduction.html#separable-case-hard-margin",
    "title": "1 Support Vector Machines (SVM)",
    "section": "1.2 Separable Case (Hard-Margin)",
    "text": "1.2 Separable Case (Hard-Margin)\n\nMathematical Formulation\nTraining Sample: S=\\{(\\pmb{x_1},y_1),\\ldots,(\\pmb{x_m},y_m)\\}\n\neach \\pmb{x_i} is an n\\times 1 feature vector\nsize m\nI.I.D.\n\\pmb{x}=(x_1,\\ldots,x_n) is a vector in \\mathbb{R}^n\ny\\in\\{-1, +1\\}\n\nObjective: Look for a hyperplane \\pmb{w}\\cdot\\pmb{x}+b=0 with the normal vector \\pmb{w} such that\n\nall sample points are on the correct side of the hyperplane, i.e., y_i(\\pmb{\\pmb{w}\\cdot\\pmb{x_i}}+b)\\geq0\\text{ for all }i\\in[m]\nthe margin (distance to the closed sample point) \\rho=\\min_{i\\in[m]}\\frac{|\\pmb{w}\\cdot\\pmb{x_i}+b|}{\\|\\pmb{w}\\|} is maximum\n\n\n\n\nThe hyperplane\n\n\n\n\nThe Solution\nThe good news is a unique solution hyperplane exists—so long as the sample points are linearly separable.\nIn that case, the solution \\pmb{w}^* is a linear combination of the training set vectors: \n\\pmb{w^*}=\\alpha_1\\pmb{x_1}+\\ldots+\\alpha_m\\pmb{x_m}.\n If the i-th training vector appears in the above linear combination (i.e., \\alpha_i\\neq0), then it’s called a support vector. Moreover, for any such support vector: \nb^*=y_i-\\sum_{j=1}^m\\alpha_j y_j (\\pmb{x}_j\\cdot\\pmb{x_i}).\n\nFor a test data point with feature vector \\pmb{x}, we classify using the following rule: \n\\pmb{x}\\mapsto\\text{sign}(\\pmb{w^*}\\cdot\\pmb{x}+b^*)\n=\\text{sgn}\\left(\\sum_{i=1}^m\\alpha_iy_i\\langle\\pmb{x_i},\\pmb{x}\\rangle+b^*\\right).\n\n\n\n\n\n\n\nSolving the Optimization Problem\n\n\n\n\n\nThe primal problem is to find a hyperplane (given by the normal vector \\pmb{w}\\in\\mathbb{R}^n and b\\in\\mathbb{R}) so that \n\\min_{\\pmb{w},b}\\frac{1}{2}\\|\\pmb{w}\\|^2\n subject to: y_i(\\pmb{w}\\cdot\\pmb{x_i}+b)\\geq1\\text{ for all }i\\in[m] This is a convex optimization problem with a unique solution.\nHowever, in order to get the optimal solution, we consider the dual problem. For more details Mohri, Rostamizadeh, and Talwalkar (2018).\n\n\n\n\n\n\n\n\n\nTip",
    "crumbs": [
      "Lecture Notes",
      "Support Vector Machines (SVM)"
    ]
  },
  {
    "objectID": "notes/introduction.html#non-separable-case-soft-margin",
    "href": "notes/introduction.html#non-separable-case-soft-margin",
    "title": "1 Support Vector Machines (SVM)",
    "section": "1.3 Non-Separable Case (Soft-Margin)",
    "text": "1.3 Non-Separable Case (Soft-Margin)\nIn real applications, the sample points are not separable. In that case, we allow for exceptions.\nWe would not mind a few exceptional sample points lying inside the maximum margin or even on the wrong side of the margin. However, the less number of exceptions, the better.\nWe toss a hyper-parameter C\\geq0 (known as the regularization parameter) in to our optimization.\n\n\n\n\n\n\nSolving with Slack Variables\n\n\n\n\n\nThe primal problem is to find a hyperplane (given by the normal vector \\pmb{w}\\in\\mathbb{R}^n and b\\in\\mathbb{R}) so that \n\\min_{\\pmb{w},b,\\pmb{\\xi}}\\frac{1}{2}\\|\\pmb{w}\\|^2 + C\\sum_{i=1}^m\\xi^2_i\n subject to: y_i(\\pmb{w}\\cdot\\pmb{x_i}+b)\\geq1-\\xi_i\\text{ for all }i\\in[m] Here, the \\pmb{\\xi}=(\\xi_1,\\ldots,\\xi_m) is called the slack.\nThis is a also convex optimization problem with a unique solution.\nHowever, in order to get the optimal solution, we consider the dual problem. For more details (Mohri, Rostamizadeh, and Talwalkar 2018, chap. 5).\n\n\n\nConsequently the objective of the optimization becomes two-fold:\n\nmaximize the margin\nlimit the total amount of slack.\n\n\n\n\n\n\n\nAbout the C\n\n\n\nThe regularization parameter C controls the width of the margin. The smaller C gets, the wider the margin becomes.\nSome of the common choices are 0.001, 0.01, 0.1, 1.0, 100, etc.\n\n\n\n\n\n\n\n\nTip",
    "crumbs": [
      "Lecture Notes",
      "Support Vector Machines (SVM)"
    ]
  },
  {
    "objectID": "notes/introduction.html#kernel-method",
    "href": "notes/introduction.html#kernel-method",
    "title": "1 Support Vector Machines (SVM)",
    "section": "1.4 Kernel Method",
    "text": "1.4 Kernel Method",
    "crumbs": [
      "Lecture Notes",
      "Support Vector Machines (SVM)"
    ]
  },
  {
    "objectID": "notes/introduction.html#multi-class-classification",
    "href": "notes/introduction.html#multi-class-classification",
    "title": "1 Support Vector Machines (SVM)",
    "section": "1.5 Multi-class Classification",
    "text": "1.5 Multi-class Classification",
    "crumbs": [
      "Lecture Notes",
      "Support Vector Machines (SVM)"
    ]
  },
  {
    "objectID": "notes/introduction.html#appendix",
    "href": "notes/introduction.html#appendix",
    "title": "1 Support Vector Machines (SVM)",
    "section": "1.6 Appendix",
    "text": "1.6 Appendix\n\npivots = Object({ x1: 2, y1: 2, x2: 4, y2: 4 });\ncompute_c = function (points) {\n  return points.y1 - compute_m(points) * points.x1;\n}\ncompute_m = function (points) {\n  return (points.y1 - points.y2) / (points.x1 - points.x2);\n}\nupdate = function (svg, X, Y, zrange, strech, sep) {\n  const xdomain = X.domain();\n  const ydomain = Y.domain();\n  const m = compute_m(pivots);\n  const c = compute_c(pivots);\n\n  svg\n    .select(\"#line\")\n    .attr(\"stroke\", sep ? \"black\" : \"lightgray\")\n    .attr(\n      \"x1\",\n      m * xdomain[0] + c &gt;= ydomain[0] ? X(xdomain[0]) : X((ydomain[0] - c) / m)\n    )\n    .attr(\n      \"y1\",\n      m * xdomain[0] + c &gt;= ydomain[0] ? Y(m * xdomain[0] + c) : Y(ydomain[0])\n    )\n    .attr(\n      \"x2\",\n      m * xdomain[1] + c &gt;= ydomain[0] ? X(xdomain[1]) : X((ydomain[0] - c) / m)\n    )\n    .attr(\n      \"y2\",\n      m * xdomain[1] + c &gt;= ydomain[0] ? Y(m * xdomain[1] + c) : Y(ydomain[0])\n    );\n\n  svg\n    .select(\"#R1\")\n    .attr(\n      \"points\",\n      [\n        [X(xdomain[0]), Y(m * xdomain[0] + c)],\n        [X(xdomain[1]), Y(m * xdomain[1] + c)],\n        [X(xdomain[1]), Y(m * xdomain[1] + c + strech)],\n        [X(xdomain[0]), Y(m * xdomain[0] + c + strech)]\n      ].join(\",\")\n    )\n    .style(\"fill\", zrange[0])\n    .style(\"opacity\", 0.1);\n  svg\n    .select(\"#R2\")\n    .attr(\n      \"points\",\n      [\n        [X(xdomain[0]), Y(m * xdomain[0] + c)],\n        [X(xdomain[1]), Y(m * xdomain[1] + c)],\n        [X(xdomain[1]), Y(m * xdomain[1] + c - strech)],\n        [X(xdomain[0]), Y(m * xdomain[0] + c - strech)]\n      ].join(\",\")\n    )\n    .style(\"fill\", zrange[1])\n    .style(\"opacity\", 0.1);\n\n  svg\n    .select(\"#M1\")\n    .attr(\n      \"points\",\n      [\n        [X(xdomain[0]), Y(m * xdomain[0] + c)],\n        [X(xdomain[1]), Y(m * xdomain[1] + c)],\n        [X(xdomain[1]), Y(m * xdomain[1] + c + margin * Math.sqrt(1 + m * m))],\n        [X(xdomain[0]), Y(m * xdomain[0] + c + margin * Math.sqrt(1 + m * m))]\n      ].join(\",\")\n    )\n    .style(\"fill\", zrange[0])\n    .style(\"opacity\", 0.3);\n\n  svg\n    .select(\"#M2\")\n    .attr(\n      \"points\",\n      [\n        [X(xdomain[0]), Y(m * xdomain[0] + c)],\n        [X(xdomain[1]), Y(m * xdomain[1] + c)],\n        [X(xdomain[1]), Y(m * xdomain[1] + c - margin * Math.sqrt(1 + m * m))],\n        [X(xdomain[0]), Y(m * xdomain[0] + c - margin * Math.sqrt(1 + m * m))]\n      ].join(\",\")\n    )\n    .style(\"fill\", zrange[1])\n    .style(\"opacity\", 0.3);\n}\nseparates = function (data, x, y, z) {\n  const m = compute_m(pivots);\n  const c = compute_c(pivots);\n  if (data[0][y] &gt; m * data[0][x] + c)\n    return data.every(\n      (d) =&gt;\n        (d[z] == data[0][z] && d[y] &gt; m * d[x] + c) ||\n        (d[z] != data[0][z] && d[y] &lt; m * d[x] + c)\n    );\n  else if (data[0][y] &lt; m * data[0][x] + c)\n    return data.every(\n      (d) =&gt;\n        (d[z] == data[0][z] && d[y] &lt; m * d[x] + c) ||\n        (d[z] != data[0][z] && d[y] &gt; m * d[x] + c)\n    );\n  else return false;\n}\ndraw = function (data, args = {}) {\n  // Declare the chart dimensions and margins.\n  const width = args.width || 500;\n  const height = args.width || 400;\n  const marginTop = args.marginTop || 5;\n  const marginRight = args.marginRight || 20;\n  const marginBottom = args.marginBottom || 50;\n  const marginLeft = args.marginLeft || 40;\n  const x = args.x || \"x\";\n  const y = args.y || \"y\";\n  const z = args.z || \"z\";\n  const xdomain = args.xdomain || [0, d3.max(data, (d) =&gt; d[x])];\n  const ydomain = args.ydomain || [0, d3.max(data, (d) =&gt; d[y])];\n  const zdomain = args.zdomain || [0, 1];\n  const zrange = args.zrange || [\"red\", \"blue\"];\n  const m = compute_m(pivots);\n  const c = compute_c(pivots);\n\n  // Declare the x (horizontal position) scale.\n  const X = d3\n    .scaleLinear()\n    .domain(xdomain)\n    .range([marginLeft, width - marginRight]);\n\n  // Declare the y (vertical position) scale.\n  const Y = d3\n    .scaleLinear()\n    .domain(ydomain)\n    .range([height - marginBottom, marginTop]);\n\n  // Declare the fill axis\n  const Z = d3.scaleOrdinal().domain(zdomain).range(zrange);\n\n  // Create the SVG container.\n  const svg = d3.create(\"svg\").attr(\"width\", width).attr(\"height\", height);\n\n  // Add the x-axis.\n  svg\n    .append(\"g\")\n    .attr(\"transform\", `translate(0,${height - marginBottom})`)\n    .call(d3.axisBottom(X));\n  svg.append(\"text\")\n    .attr(\"text-anchor\", \"end\")\n    .attr(\"x\", width - 50)\n    .attr(\"y\", height - 20 )\n    .text(\"X_1: age →\");\n  svg.append(\"text\")\n    .attr(\"text-anchor\", \"end\")\n    .attr(\"transform\", \"rotate(-90)\")\n    .attr(\"y\",  13)\n    .attr(\"x\", -20)\n    .text(\"X_2: education (yrs) →\")\n\n  // Add the y-axis.\n  svg\n    .append(\"g\")\n    .attr(\"transform\", `translate(${marginLeft},0)`)\n    .call(d3.axisLeft(Y));\n\n  const regions = svg.append(\"g\").attr(\"id\", \"regions\");\n  regions.append(\"polygon\").attr(\"id\", \"R1\");\n  regions.append(\"polygon\").attr(\"id\", \"R2\");\n\n  // Add the dots\n  svg\n    .append(\"g\")\n    .selectAll(\"dot\")\n    .data(data)\n    .enter()\n    .append(\"circle\")\n    .attr(\"cx\", (d) =&gt; X(d[x]))\n    .attr(\"cy\", (d) =&gt; Y(d[y]))\n    .attr(\"r\", 3)\n    .style(\"fill\", (d) =&gt; Z(d[z]));\n\n  // Show Margins\n  svg.append(\"g\").attr(\"id\", \"margins\");\n  regions.append(\"polygon\").attr(\"id\", \"M1\");\n  regions.append(\"polygon\").attr(\"id\", \"M2\");\n\n  // Add the line\n  const line = svg.append(\"g\");\n\n  line\n    .append(\"line\")\n    .attr(\"id\", \"line\")\n    .attr(\"stroke-width\", 3)\n    .attr(\"path-length\", 10);\n\n  update(svg, X, Y, zrange, Math.max(width, height), separates(data, x, y, z));\n\n  // Draw the pivot points\n  line\n    .append(\"circle\")\n    .attr(\"cx\", X(pivots.x1))\n    .attr(\"cy\", Y(pivots.y1))\n    .attr(\"r\", 7)\n    .style(\"fill\", \"white\")\n    .style(\"stroke\", \"lightgrey\")\n    .style(\"stroke-width\", 3)\n    .call(\n      d3.drag().on(\"drag\", function (event, d) {\n        pivots.x1 = X.invert(event.x);\n        pivots.y1 = Y.invert(event.y);\n\n        d3.select(this).attr(\"cx\", X(pivots.x1)).attr(\"cy\", Y(pivots.y1));\n        update(\n          svg,\n          X,\n          Y,\n          zrange,\n          Math.max(width, height),\n          separates(data, x, y, z)\n        );\n      })\n    );\n  line\n    .append(\"circle\")\n    .attr(\"cx\", X(pivots.x2))\n    .attr(\"cy\", Y(pivots.y2))\n    .attr(\"r\", 7)\n    .style(\"fill\", \"white\")\n    .style(\"stroke\", \"lightgrey\")\n    .style(\"stroke-width\", 3)\n    .call(\n      d3.drag().on(\"drag\", function (event, d) {\n        pivots.x2 = X.invert(event.x);\n        pivots.y2 = Y.invert(event.y);\n\n        d3.select(this).attr(\"cx\", X(pivots.x2)).attr(\"cy\", Y(pivots.y2));\n        update(\n          svg,\n          X,\n          Y,\n          zrange,\n          Math.max(width, height),\n          separates(data, x, y, z)\n        );\n      })\n    );\n\n  // Draw regions\n  if (showRegions) {\n    regions.attr(\"visibility\", \"visible\");\n    regions.attr(\"visibility\", \"visible\");\n  } else {\n    regions.attr(\"visibility\", \"hidden\");\n    regions.attr(\"visibility\", \"hidden\");\n  }\n\n  // Draw line\n  if (showLine) {\n    line.attr(\"visibility\", \"visible\");\n    line.attr(\"visibility\", \"visible\");\n  } else {\n    line.attr(\"visibility\", \"hidden\");\n    line.attr(\"visibility\", \"hidden\");\n  }\n\n  // Return the SVG element.\n  return svg.node();\n}\ngenerateData = function (min = -1, max = 1, n = 10, method = \"linear\") {\n  const rand = d3.randomUniform(min, max);\n  const pivots = {\n    x1: rand(),\n    x2: rand(),\n    y1: rand(),\n    y2: rand()\n  };\n  const a = d3.randomUniform(0, 0.3)();\n  const b = d3.randomUniform(0, 0.3)();\n\n  return d3.range(n).map((d) =&gt; {\n    const x = rand();\n    const y = rand();\n    let z;\n    if (method == \"linear\")\n      z = y - compute_m(pivots) * x - compute_c(pivots) &gt; 0 ? 0 : 1;\n    else if (method == \"quad\") {\n      z = a * x * x + b * y * y - 1 &gt; 0 ? 0 : 1;\n    }\n    return {\n      x: x,\n      y: y,\n      z: z\n    };\n  });\n}\ndata1 = generateData(-5, 5, 15, \"linear\")",
    "crumbs": [
      "Lecture Notes",
      "Support Vector Machines (SVM)"
    ]
  },
  {
    "objectID": "notes/algorithms.html",
    "href": "notes/algorithms.html",
    "title": "1 Analysis of Algorithms",
    "section": "",
    "text": "“Programming is the art of telling another human being what one wants the computer to do.”\n– Donald E. Knuth"
  },
  {
    "objectID": "notes/algorithms.html#computational-problems",
    "href": "notes/algorithms.html#computational-problems",
    "title": "1 Analysis of Algorithms",
    "section": "1.1 Computational Problems",
    "text": "1.1 Computational Problems\nLet us first define a computational problem. A (computational) problem is a well-specified task stated in general terms, specifying the desired output for any acceptable input to the problem.\nFor example, sorting a list of numbers is a computational problem. It stipulates that an acceptable input must be a list or collection of real numbers, and the desired output must be a list with the same numbers but arranged in increasing (equivalently decreasing) order.\nKeep in mind that the task of sorting a specific list (e.g. [0, -1, 2, 0]) should not be called a computational problem, since it is not given in general terms. When considering such a specific input to the problem, we may rather call it an instance of the sorting problem as discussed in the previous paragraph.\nFormally, we define the sorting problem as follows:\n\nInput: an array of numbers A=\\langle a_1, a_2, \\ldots, a_n\\rangle.\nOutput: A reordering or permutation \\langle a_1', a_2', \\ldots, a_n'\\rangle of the array A such that a_1'\\leq a_2'\\leq\\ldots\\leq a_n.\n\n\nExercise 1 Name five computational problems that we come across in our daily lives.\n\n\n\n\n\n\n\nSolution:\n\n\n\n\n\n\nadding two numbers\nsorting an array\nfinding the minimum value is an array\nsearching a query value in an array\nfinding the shortest path between two vertices in a graph\n\n\n\n\nWe use algorithms to solve computational problems."
  },
  {
    "objectID": "notes/algorithms.html#what-is-an-algorithm",
    "href": "notes/algorithms.html#what-is-an-algorithm",
    "title": "1 Analysis of Algorithms",
    "section": "1.2 What is an Algorithm?",
    "text": "1.2 What is an Algorithm?\nAn algorithm is any well-defined computational procedure that takes some value (or set of values) as input and produces some value as output. Thus, an algorithm is a sequence of (finite) computational steps that transform its input to an output.\nWe may have different algorithms to solve the same computational problem. However, they often differ dramatically in their efficiency.\n\nWriting Pseudocode\nIn this course, we mostly rely on pseudocodes for describing algorithms. Their implementation in Python will be an less significant step. Pseudocode delineates the logic of an algorithm without explicitly using the syntax of a programming language.\n\n\n\n\n\n\nConvention\n\n\n\n\n\nWe use the following conventions in our pseudocode:\n\nIndentation indicates block structure.\nThe looping constructs while, for, and repeat-until and the if-else conditional construct have interpretations similar to Python.\nThe symbol // indicates that the remainder of the line is a comment.\nVariables (such as i, j , and key) are local to the given procedure. We won’t use global variables without explicit indication.\nWe access array elements by specifying the array name followed by the index in square brackets. For example, A[i] indicates the ith element of the array A.\nWe typically organize compound data into objects, which are composed of attributes.\nWe pass parameters to a procedure by value: the called procedure receives its own copy of the parameters, and if it assigns a value to a parameter, the change is not seen by the calling procedure\nA return statement immediately transfers control back to the point of call in the calling procedure.\nThe boolean operators and and or are short circuiting.\n\n\n\n\nWe give a few examples first."
  },
  {
    "objectID": "notes/algorithms.html#example-algorithms",
    "href": "notes/algorithms.html#example-algorithms",
    "title": "1 Analysis of Algorithms",
    "section": "1.3 Example Algorithms",
    "text": "1.3 Example Algorithms\nLet us develop and analyze a few example algorithms.\n\nSUM-ARRAY\nDescription: SUM-ARRAY is a computational problem that computes the sum of the n numbers in array A[0:n-1].\n\nInput: An array or list A[0:n-1] containing n numbers.\nOutput: The sum of the numbers in A[0:n-1].\n\n\nExample 1 If the input array is A = &lt;1, 0, -1, 2&gt;, then the return must be 1 + 0 + (-1) + 2 = 2.\n\n\n\n\n\n\n\nPseudocode\n\n\n\n\n\n\n\n\\begin{algorithm} \\caption{SUM-ARRAY} \\begin{algorithmic} \\Procedure{SUM-ARRAY}{$A[0:n-1]$} \\State $sum = 0$ \\For{$i = 0$ \\To $n-1$} \\State $sum = sum + A[i]$ \\State $i = i + 1$ \\EndFor \\Return $sum$ \\EndProcedure \\end{algorithmic} \\end{algorithm}\n\n\n\n\n\n\nExercise 2 (Dry)-run SUM-ARRAY on the input A=&lt;-2, 3, 5, 0, -1&gt;.\n\n\nExercise 3 Implement SUM-ARRAY in Python.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ndef SUM_ARRAY(arr):\n    sum = 0\n    n = len(arr)\n    for i in range(n):\n        sum = sum + arr[i]\n    return sum\n\n\n\n\n\n\nSUM-ARRAY-POS\nDescription: SUM-ARRAY-POS is a computational problem that computes the sum of the positive numbers in an array A[0:n-1] of size n.\n\nInput: An array or list A[0:n-1] containing n numbers.\nOutput: The sum of the positive numbers in A.\n\n\nExample 2 If the input is the array A = &lt;1, 0, -1, 2&gt;, then the return will be 1 + 2 = 3.\n\n\n\n\n\n\n\nPseudocode\n\n\n\n\n\n\n\n\\begin{algorithm} \\caption{SUM-ARRAY-POS} \\begin{algorithmic} \\Procedure{SUM-ARRAY-POS}{$A[0:n-1]$} \\State $sum = 0$ \\For{$i = 0$ \\To $n-1$} \\If{$A[i] &gt; 0$} \\State $sum = sum + A[i]$ \\EndIf \\State $i = i + 1$ \\EndFor \\Return $sum$ \\EndProcedure \\end{algorithmic} \\end{algorithm}\n\n\n\n\n\n\nExercise 4 (Dry)-run SUM-ARRAY-POS on the input A=&lt;-2, 3, 5, 0, -1&gt;.\n\n\nExercise 5 Implement SUM-ARRAY-POS in Python.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ndef SUM_ARRAY_POS(arr):\n    sum = 0\n    n = len(arr)\n    for i in range(n):\n      if arr[i] &gt; 0:\n        sum = sum + arr[i]\n    return sum\n\n\n\n\n\n\nSEARCH-ARRAY\nDescription: SEARCH-ARRAY is a computational problem that finds an element in an array.\n\nInput: An array or list A[0:n-1] containing n numbers and a query value x.\nOutput: An first index i such that A[i]=x, otherwise -1.\n\n\nExample 3 If the inputs are the array A = &lt;1, 0, -1, 2&gt; and the value x = 0, then the return will be 1.\n\n\n\n\n\n\n\nPseudocode\n\n\n\n\n\n\n\n\\begin{algorithm} \\caption{SEARCH-ARRAY} \\begin{algorithmic} \\Procedure{SEARCH-ARRAY}{$A[0:n-1]$, $x$} \\For{$i = 0$ \\To $n-1$} \\If{$A[i] = x$} \\Return $i$ \\EndIf \\State $i = i + 1$ \\EndFor \\Return $-1$ \\EndProcedure \\end{algorithmic} \\end{algorithm}\n\n\n\n\n\n\nExercise 6 (Dry)-run SEARCH-ARRAY on the input A=&lt;-2, 3, 5, 0, -1&gt; and x=4.\n\n\nExercise 7 Implement SEARCH-ARRAY in Python.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ndef SEARCH_ARRAY(arr, x):\n    n = len(arr)\n    for i in range(n):\n        if arr[i] == x:\n          return i\n    return -1\n\n\n\n\n\nExercise 8 (MIN-ARRAY) Develop an algorithm to find the minimum value in an array A[0:n-1].\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\\begin{algorithm} \\caption{MIN-ARRAY} \\begin{algorithmic} \\Procedure{MIN-ARRAY}{$A[0:n-1]$} \\State $min=A[0]$ \\For{$i = 1$ \\To $n-1$} \\If{$A[i] &lt; min$} \\State $min=A[i]$ \\EndIf \\State $i = i + 1$ \\EndFor \\Return $min$ \\EndProcedure \\end{algorithmic} \\end{algorithm}\n\n\n\n\n\n\nExercise 9 (SUM-MATRIX) Develop an algorithm for summing all the elements in a square matrix A=[a_{i,j}] of size n\\times n.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\\begin{algorithm} \\caption{SUM-MATRIX} \\begin{algorithmic} \\Procedure{SUM-MATRIX}{$A[0:n-1][0:n-1]$} \\State $min=0$ \\For{$i = 0$ \\To $n-1$} \\For{$j = 0$ \\To $n-1$} \\State $sum=sum + A[i][j]$ \\State $j = j + 1$ \\EndFor \\State $i = i + 1$ \\EndFor \\Return $sum$ \\EndProcedure \\end{algorithmic} \\end{algorithm}"
  },
  {
    "objectID": "notes/algorithms.html#analyzing-algorithms",
    "href": "notes/algorithms.html#analyzing-algorithms",
    "title": "1 Analysis of Algorithms",
    "section": "1.4 Analyzing Algorithms",
    "text": "1.4 Analyzing Algorithms\nAnalyzing an algorithm means to predict the computational resources (e.g. computational time, memory) needed to run the algorithm on different instances or inputs. But how do we compute the running time of an algorithm?\n\nExercise 10 Do you think it is easy to compute the time taken by an algorithm?\n\nAlthough such a task is almost impossible, we can still pretty easily analyze the computational time of an algorithm—at least conceptually!\nLet’s use first review our basic intuition behind running-time of an algorithm:\n\nit should depend on the input size\nthe larger the input size, the more time needed\nrunning-time should be machine and compiler dependent\n\nIn order to make the task easier, we sum the running times for each statement executed. We usually denote the running time of an algorithm on an input of size n by T(n).\n\nSUM-ARRAY\nFor each line in the algorithm, we assign a cost symbolically, then count how many times the line is visited. This is summarized in the following table.\n\n\n\nLine\nCost\nTimes\n\n\n\n\n2\nc_1\n1\n\n\n3\nc_2\nn\n\n\n4\nc_3\nn\n\n\n5\nc_4\nn\n\n\n6\nc_5\nn\n\n\n7\nc_5\n1\n\n\n\nSo, T(n)=(c_1 + c_5) + (c_2 + c_3 + c_4)n=a + bn, where a\\coloneqq(c_1 + c_5) and b\\coloneqq(c_2 + c_3 + c_4). This is a linear function of the input size.\n\n\nSUM-MATRIX\n\nExercise 11 Analyze the running time of SUM-MATRIX.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe refer to the pseudocode of SUM-MATRIX for this analysis.\n\n\n\nLine\nCost\nTimes\n\n\n\n\n2\nc_1\n1\n\n\n3\nc_2\nn\n\n\n4\nc_3\nn\\times n\n\n\n5\nc_4\nn\\times n\n\n\n6\nc_5\nn\\times n\n\n\n8\nc_6\nn\n\n\n10\nc_7\n1\n\n\n\nSo, the running time T(n)=(c_1 + c_7) + (c_2 + c_6)n + (c_3 + c_4 + c_5)n^2=a + bn+cn^2, where a\\coloneqq(c_1 + c_7), b\\coloneqq(c_2 + c_6), and c\\coloneqq(c_3 + c_4 + c_5). This is a quadratic function of the input size.\n\n\n\n\n\nSEARCH-ARRAY\nFor this analysis, we first note the best case and the worst case inputs.\nThe best case is when the value x is the first element of the input array A. The search halts in the very first iteration. In this case, the running time is T(n)=a for some constant a.\nThe worst case occurs when the value x is the last element of the input array A or x does not belong to A. The search, in this case, loops through all the elements of the array. In this case, the running time is T(n)=a + bn for some constants a,b.\n\n\n\nTuring Machine taken from Markoš, Kelemen, 2004"
  }
]