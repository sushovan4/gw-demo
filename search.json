[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning",
    "section": "",
    "text": "Welcome, Everyone!",
    "crumbs": [
      "Welcome, Everyone!"
    ]
  },
  {
    "objectID": "index.html#who-is-the-instructor",
    "href": "index.html#who-is-the-instructor",
    "title": "Machine Learning",
    "section": "Who is the instructor?",
    "text": "Who is the instructor?\nName: Dr. Sushovan Majhi\nEmail: s.majhi@gwu.edu\nHomepage: smajhi.com",
    "crumbs": [
      "Welcome, Everyone!"
    ]
  },
  {
    "objectID": "notes/introduction.html",
    "href": "notes/introduction.html",
    "title": "1  Support Vector Machines (SVM)",
    "section": "",
    "text": "1.1 Maximum-Margin Classifier",
    "crumbs": [
      "Supervised",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Support Vector Machines (SVM)</span>"
    ]
  },
  {
    "objectID": "notes/introduction.html#maximum-margin-classifier",
    "href": "notes/introduction.html#maximum-margin-classifier",
    "title": "1  Support Vector Machines (SVM)",
    "section": "",
    "text": "A Motivating Example\nLet us ask the curious question: can we learn to predict party affiliation of a US national from their age and years of education?\n\nNumber of features: n=2.\nFeature space: \\mathcal{X} is a subset of \\mathbb{R}^2.\nTarget space: \\mathcal{Y}={RED, BLUE}.\n\n\n\n\n\ndraw(\n  data1.map((d) =&gt; ({ x: d.x, y: d.y, z: d.z })),\n  {\n    x: \"x\",\n    y: \"y\",\n    xdomain: [-6, 6],\n    ydomain: [-6, 6]\n  }\n);\n\n\n\n\n\n\n\n\n\nviewof margin = Inputs.range([0, 5], {\n  value: 0,\n  step: 0.01,\n  label: \"Margin\"\n});\nviewof showLine = Inputs.toggle({ value: false, label: \"Show line\" });\nviewof showRegions = Inputs.toggle({ value: false, label: \"Classified regions\" });\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn the demo, we are trying to:\n\nseparate the separate our training labelled examples by a line (more generally a hyperplane).\nonce we have learned a separating hyperplane, an unseen test point can be classified based on which side of the hyperplane the point is.\n\n\n\n\n\n\n\nSummary\n\n\n\nIn summary, the SVM tries to learn from a sample a linear decision boundary that fully separates the training data—when possible.\n\n\n\n\nPros\n\nVery intuitive\nEfficient algorithms are available\n\nLinear Programming Problem\nPerceptron (Rosenblatt 1958)\n\nEasily generalized to a higher-dimensional feature space\n\ndecision boundary: line (n=2), plane (n=3), hyperplane (n\\geq3)\n\n\n\n\nCons\n\nThere are infinitely many separating hyperplanes\n\nmaximum-margin\n\nThe data may not be always linearly separable\n\nsoft-margin\nKernel methods",
    "crumbs": [
      "Supervised",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Support Vector Machines (SVM)</span>"
    ]
  },
  {
    "objectID": "notes/introduction.html#separable-case-hard-margin",
    "href": "notes/introduction.html#separable-case-hard-margin",
    "title": "1  Support Vector Machines (SVM)",
    "section": "1.2 Separable Case (Hard-Margin)",
    "text": "1.2 Separable Case (Hard-Margin)\n\nMathematical Formulation\nTraining Sample: S=\\{(\\pmb{x_1},y_1),\\ldots,(\\pmb{x_m},y_m)\\}\n\neach \\pmb{x_i} is an n\\times 1 feature vector\nsize m\nI.I.D.\n\\pmb{x}=(x_1,\\ldots,x_n) is a vector in \\mathbb{R}^n\ny\\in\\{-1, +1\\}\n\nObjective: Look for a hyperplane \\pmb{w}\\cdot\\pmb{x}+b=0 with the normal vector \\pmb{w} such that\n\nall sample points are on the correct side of the hyperplane, i.e., y_i(\\pmb{\\pmb{w}\\cdot\\pmb{x_i}}+b)\\geq0\\text{ for all }i\\in[m]\nthe margin (distance to the closed sample point) \\rho=\\min_{i\\in[m]}\\frac{|\\pmb{w}\\cdot\\pmb{x_i}+b|}{\\|\\pmb{w}\\|} is maximum\n\n\n\n\nThe hyperplane\n\n\n\n\nThe Solution\nThe good news is a unique solution hyperplane exists—so long as the sample points are linearly separable.\nIn that case, the solution \\pmb{w}^* is a linear combination of the training set vectors: \n\\pmb{w^*}=\\alpha_1\\pmb{x_1}+\\ldots+\\alpha_m\\pmb{x_m}.\n If the i-th training vector appears in the above linear combination (i.e., \\alpha_i\\neq0), then it’s called a support vector. Moreover, for any such support vector: \nb^*=y_i-\\sum_{j=1}^m\\alpha_j y_j (\\pmb{x}_j\\cdot\\pmb{x_i}).\n\nFor a test data point with feature vector \\pmb{x}, we classify using the following rule: \n\\pmb{x}\\mapsto\\text{sign}(\\pmb{w^*}\\cdot\\pmb{x}+b^*)\n=\\text{sgn}\\left(\\sum_{i=1}^m\\alpha_iy_i\\langle\\pmb{x_i},\\pmb{x}\\rangle+b^*\\right).\n\n\n\n\n\n\n\nSolving the Optimization Problem\n\n\n\n\n\nThe primal problem is to find a hyperplane (given by the normal vector \\pmb{w}\\in\\mathbb{R}^n and b\\in\\mathbb{R}) so that \n\\min_{\\pmb{w},b}\\frac{1}{2}\\|\\pmb{w}\\|^2\n subject to: y_i(\\pmb{w}\\cdot\\pmb{x_i}+b)\\geq1\\text{ for all }i\\in[m] This is a convex optimization problem with a unique solution.\nHowever, in order to get the optimal solution, we consider the dual problem. For more details Mohri, Rostamizadeh, and Talwalkar (2018).\n\n\n\n\n\n\n\n\n\nTip",
    "crumbs": [
      "Supervised",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Support Vector Machines (SVM)</span>"
    ]
  },
  {
    "objectID": "notes/introduction.html#non-separable-case-soft-margin",
    "href": "notes/introduction.html#non-separable-case-soft-margin",
    "title": "1  Support Vector Machines (SVM)",
    "section": "1.3 Non-Separable Case (Soft-Margin)",
    "text": "1.3 Non-Separable Case (Soft-Margin)\nIn real applications, the sample points are not separable. In that case, we allow for exceptions.\nWe would not mind a few exceptional sample points lying inside the maximum margin or even on the wrong side of the margin. However, the less number of exceptions, the better.\nWe toss a hyper-parameter C\\geq0 (known as the regularization parameter) in to our optimization.\n\n\n\n\n\n\nSolving with Slack Variables\n\n\n\n\n\nThe primal problem is to find a hyperplane (given by the normal vector \\pmb{w}\\in\\mathbb{R}^n and b\\in\\mathbb{R}) so that \n\\min_{\\pmb{w},b,\\pmb{\\xi}}\\frac{1}{2}\\|\\pmb{w}\\|^2 + C\\sum_{i=1}^m\\xi^2_i\n subject to: y_i(\\pmb{w}\\cdot\\pmb{x_i}+b)\\geq1-\\xi_i\\text{ for all }i\\in[m] Here, the \\pmb{\\xi}=(\\xi_1,\\ldots,\\xi_m) is called the slack.\nThis is a also convex optimization problem with a unique solution.\nHowever, in order to get the optimal solution, we consider the dual problem. For more details (Mohri, Rostamizadeh, and Talwalkar 2018, chap. 5).\n\n\n\nConsequently the objective of the optimization becomes two-fold:\n\nmaximize the margin\nlimit the total amount of slack.\n\n\n\n\n\n\n\nAbout the C\n\n\n\nThe regularization parameter C controls the width of the margin. The smaller C gets, the wider the margin becomes.\nSome of the common choices are 0.001, 0.01, 0.1, 1.0, 100, etc.\n\n\n\n\n\n\n\n\nTip",
    "crumbs": [
      "Supervised",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Support Vector Machines (SVM)</span>"
    ]
  },
  {
    "objectID": "notes/introduction.html#kernel-method",
    "href": "notes/introduction.html#kernel-method",
    "title": "1  Support Vector Machines (SVM)",
    "section": "1.4 Kernel Method",
    "text": "1.4 Kernel Method",
    "crumbs": [
      "Supervised",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Support Vector Machines (SVM)</span>"
    ]
  },
  {
    "objectID": "notes/introduction.html#multi-class-classification",
    "href": "notes/introduction.html#multi-class-classification",
    "title": "1  Support Vector Machines (SVM)",
    "section": "1.5 Multi-class Classification",
    "text": "1.5 Multi-class Classification",
    "crumbs": [
      "Supervised",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Support Vector Machines (SVM)</span>"
    ]
  },
  {
    "objectID": "notes/introduction.html#appendix",
    "href": "notes/introduction.html#appendix",
    "title": "1  Support Vector Machines (SVM)",
    "section": "1.6 Appendix",
    "text": "1.6 Appendix\n\npivots = Object({ x1: 2, y1: 2, x2: 4, y2: 4 });\ncompute_c = function (points) {\n  return points.y1 - compute_m(points) * points.x1;\n}\ncompute_m = function (points) {\n  return (points.y1 - points.y2) / (points.x1 - points.x2);\n}\nupdate = function (svg, X, Y, zrange, strech, sep) {\n  const xdomain = X.domain();\n  const ydomain = Y.domain();\n  const m = compute_m(pivots);\n  const c = compute_c(pivots);\n\n  svg\n    .select(\"#line\")\n    .attr(\"stroke\", sep ? \"black\" : \"lightgray\")\n    .attr(\n      \"x1\",\n      m * xdomain[0] + c &gt;= ydomain[0] ? X(xdomain[0]) : X((ydomain[0] - c) / m)\n    )\n    .attr(\n      \"y1\",\n      m * xdomain[0] + c &gt;= ydomain[0] ? Y(m * xdomain[0] + c) : Y(ydomain[0])\n    )\n    .attr(\n      \"x2\",\n      m * xdomain[1] + c &gt;= ydomain[0] ? X(xdomain[1]) : X((ydomain[0] - c) / m)\n    )\n    .attr(\n      \"y2\",\n      m * xdomain[1] + c &gt;= ydomain[0] ? Y(m * xdomain[1] + c) : Y(ydomain[0])\n    );\n\n  svg\n    .select(\"#R1\")\n    .attr(\n      \"points\",\n      [\n        [X(xdomain[0]), Y(m * xdomain[0] + c)],\n        [X(xdomain[1]), Y(m * xdomain[1] + c)],\n        [X(xdomain[1]), Y(m * xdomain[1] + c + strech)],\n        [X(xdomain[0]), Y(m * xdomain[0] + c + strech)]\n      ].join(\",\")\n    )\n    .style(\"fill\", zrange[0])\n    .style(\"opacity\", 0.1);\n  svg\n    .select(\"#R2\")\n    .attr(\n      \"points\",\n      [\n        [X(xdomain[0]), Y(m * xdomain[0] + c)],\n        [X(xdomain[1]), Y(m * xdomain[1] + c)],\n        [X(xdomain[1]), Y(m * xdomain[1] + c - strech)],\n        [X(xdomain[0]), Y(m * xdomain[0] + c - strech)]\n      ].join(\",\")\n    )\n    .style(\"fill\", zrange[1])\n    .style(\"opacity\", 0.1);\n\n  svg\n    .select(\"#M1\")\n    .attr(\n      \"points\",\n      [\n        [X(xdomain[0]), Y(m * xdomain[0] + c)],\n        [X(xdomain[1]), Y(m * xdomain[1] + c)],\n        [X(xdomain[1]), Y(m * xdomain[1] + c + margin * Math.sqrt(1 + m * m))],\n        [X(xdomain[0]), Y(m * xdomain[0] + c + margin * Math.sqrt(1 + m * m))]\n      ].join(\",\")\n    )\n    .style(\"fill\", zrange[0])\n    .style(\"opacity\", 0.3);\n\n  svg\n    .select(\"#M2\")\n    .attr(\n      \"points\",\n      [\n        [X(xdomain[0]), Y(m * xdomain[0] + c)],\n        [X(xdomain[1]), Y(m * xdomain[1] + c)],\n        [X(xdomain[1]), Y(m * xdomain[1] + c - margin * Math.sqrt(1 + m * m))],\n        [X(xdomain[0]), Y(m * xdomain[0] + c - margin * Math.sqrt(1 + m * m))]\n      ].join(\",\")\n    )\n    .style(\"fill\", zrange[1])\n    .style(\"opacity\", 0.3);\n}\nseparates = function (data, x, y, z) {\n  const m = compute_m(pivots);\n  const c = compute_c(pivots);\n  if (data[0][y] &gt; m * data[0][x] + c)\n    return data.every(\n      (d) =&gt;\n        (d[z] == data[0][z] && d[y] &gt; m * d[x] + c) ||\n        (d[z] != data[0][z] && d[y] &lt; m * d[x] + c)\n    );\n  else if (data[0][y] &lt; m * data[0][x] + c)\n    return data.every(\n      (d) =&gt;\n        (d[z] == data[0][z] && d[y] &lt; m * d[x] + c) ||\n        (d[z] != data[0][z] && d[y] &gt; m * d[x] + c)\n    );\n  else return false;\n}\ndraw = function (data, args = {}) {\n  // Declare the chart dimensions and margins.\n  const width = args.width || 500;\n  const height = args.width || 400;\n  const marginTop = args.marginTop || 5;\n  const marginRight = args.marginRight || 20;\n  const marginBottom = args.marginBottom || 50;\n  const marginLeft = args.marginLeft || 40;\n  const x = args.x || \"x\";\n  const y = args.y || \"y\";\n  const z = args.z || \"z\";\n  const xdomain = args.xdomain || [0, d3.max(data, (d) =&gt; d[x])];\n  const ydomain = args.ydomain || [0, d3.max(data, (d) =&gt; d[y])];\n  const zdomain = args.zdomain || [0, 1];\n  const zrange = args.zrange || [\"red\", \"blue\"];\n  const m = compute_m(pivots);\n  const c = compute_c(pivots);\n\n  // Declare the x (horizontal position) scale.\n  const X = d3\n    .scaleLinear()\n    .domain(xdomain)\n    .range([marginLeft, width - marginRight]);\n\n  // Declare the y (vertical position) scale.\n  const Y = d3\n    .scaleLinear()\n    .domain(ydomain)\n    .range([height - marginBottom, marginTop]);\n\n  // Declare the fill axis\n  const Z = d3.scaleOrdinal().domain(zdomain).range(zrange);\n\n  // Create the SVG container.\n  const svg = d3.create(\"svg\").attr(\"width\", width).attr(\"height\", height);\n\n  // Add the x-axis.\n  svg\n    .append(\"g\")\n    .attr(\"transform\", `translate(0,${height - marginBottom})`)\n    .call(d3.axisBottom(X));\n  svg.append(\"text\")\n    .attr(\"text-anchor\", \"end\")\n    .attr(\"x\", width - 50)\n    .attr(\"y\", height - 20 )\n    .text(\"X_1: age →\");\n  svg.append(\"text\")\n    .attr(\"text-anchor\", \"end\")\n    .attr(\"transform\", \"rotate(-90)\")\n    .attr(\"y\",  13)\n    .attr(\"x\", -20)\n    .text(\"X_2: education (yrs) →\")\n\n  // Add the y-axis.\n  svg\n    .append(\"g\")\n    .attr(\"transform\", `translate(${marginLeft},0)`)\n    .call(d3.axisLeft(Y));\n\n  const regions = svg.append(\"g\").attr(\"id\", \"regions\");\n  regions.append(\"polygon\").attr(\"id\", \"R1\");\n  regions.append(\"polygon\").attr(\"id\", \"R2\");\n\n  // Add the dots\n  svg\n    .append(\"g\")\n    .selectAll(\"dot\")\n    .data(data)\n    .enter()\n    .append(\"circle\")\n    .attr(\"cx\", (d) =&gt; X(d[x]))\n    .attr(\"cy\", (d) =&gt; Y(d[y]))\n    .attr(\"r\", 3)\n    .style(\"fill\", (d) =&gt; Z(d[z]));\n\n  // Show Margins\n  svg.append(\"g\").attr(\"id\", \"margins\");\n  regions.append(\"polygon\").attr(\"id\", \"M1\");\n  regions.append(\"polygon\").attr(\"id\", \"M2\");\n\n  // Add the line\n  const line = svg.append(\"g\");\n\n  line\n    .append(\"line\")\n    .attr(\"id\", \"line\")\n    .attr(\"stroke-width\", 3)\n    .attr(\"path-length\", 10);\n\n  update(svg, X, Y, zrange, Math.max(width, height), separates(data, x, y, z));\n\n  // Draw the pivot points\n  line\n    .append(\"circle\")\n    .attr(\"cx\", X(pivots.x1))\n    .attr(\"cy\", Y(pivots.y1))\n    .attr(\"r\", 7)\n    .style(\"fill\", \"white\")\n    .style(\"stroke\", \"lightgrey\")\n    .style(\"stroke-width\", 3)\n    .call(\n      d3.drag().on(\"drag\", function (event, d) {\n        pivots.x1 = X.invert(event.x);\n        pivots.y1 = Y.invert(event.y);\n\n        d3.select(this).attr(\"cx\", X(pivots.x1)).attr(\"cy\", Y(pivots.y1));\n        update(\n          svg,\n          X,\n          Y,\n          zrange,\n          Math.max(width, height),\n          separates(data, x, y, z)\n        );\n      })\n    );\n  line\n    .append(\"circle\")\n    .attr(\"cx\", X(pivots.x2))\n    .attr(\"cy\", Y(pivots.y2))\n    .attr(\"r\", 7)\n    .style(\"fill\", \"white\")\n    .style(\"stroke\", \"lightgrey\")\n    .style(\"stroke-width\", 3)\n    .call(\n      d3.drag().on(\"drag\", function (event, d) {\n        pivots.x2 = X.invert(event.x);\n        pivots.y2 = Y.invert(event.y);\n\n        d3.select(this).attr(\"cx\", X(pivots.x2)).attr(\"cy\", Y(pivots.y2));\n        update(\n          svg,\n          X,\n          Y,\n          zrange,\n          Math.max(width, height),\n          separates(data, x, y, z)\n        );\n      })\n    );\n\n  // Draw regions\n  if (showRegions) {\n    regions.attr(\"visibility\", \"visible\");\n    regions.attr(\"visibility\", \"visible\");\n  } else {\n    regions.attr(\"visibility\", \"hidden\");\n    regions.attr(\"visibility\", \"hidden\");\n  }\n\n  // Draw line\n  if (showLine) {\n    line.attr(\"visibility\", \"visible\");\n    line.attr(\"visibility\", \"visible\");\n  } else {\n    line.attr(\"visibility\", \"hidden\");\n    line.attr(\"visibility\", \"hidden\");\n  }\n\n  // Return the SVG element.\n  return svg.node();\n}\ngenerateData = function (min = -1, max = 1, n = 10, method = \"linear\") {\n  const rand = d3.randomUniform(min, max);\n  const pivots = {\n    x1: rand(),\n    x2: rand(),\n    y1: rand(),\n    y2: rand()\n  };\n  const a = d3.randomUniform(0, 0.3)();\n  const b = d3.randomUniform(0, 0.3)();\n\n  return d3.range(n).map((d) =&gt; {\n    const x = rand();\n    const y = rand();\n    let z;\n    if (method == \"linear\")\n      z = y - compute_m(pivots) * x - compute_c(pivots) &gt; 0 ? 0 : 1;\n    else if (method == \"quad\") {\n      z = a * x * x + b * y * y - 1 &gt; 0 ? 0 : 1;\n    }\n    return {\n      x: x,\n      y: y,\n      z: z\n    };\n  });\n}\ndata1 = generateData(-5, 5, 15, \"linear\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCormen, Thomas H., Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein. 2009. Introduction to Algorithms, Third Edition. 3rd ed. The MIT Press.\n\n\nMohri, Mehryar, Afshin Rostamizadeh, and Ameet Talwalkar. 2018. Foundations of Machine Learning. 2nd ed. The MIT Press.\n\n\nRosenblatt, F. 1958. “The perceptron: A probabilistic model for information storage and organization in the brain.” Psychological Review 65 (6): 386–408. https://doi.org/10.1037/h0042519.",
    "crumbs": [
      "Supervised",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Support Vector Machines (SVM)</span>"
    ]
  }
]